{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangling & Other General Use\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# For scrapping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import urllib\n",
    "from urllib import parse\n",
    "\n",
    "# For handling files\n",
    "import os\n",
    "from os.path import isfile, join, splitext\n",
    "\n",
    "# For debugging\n",
    "from icecream import ic\n",
    "ic.configureOutput(prefix = 'Debug | ')     \n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gathering the page full HTML code (w/ Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_url(keywords_in, location_in):\n",
    "    \"\"\"Pass the parameters to an url parser\"\"\"\n",
    "    querystring = 'search?' + parse.urlencode({'keywords': keywords_in, 'location': location_in, 'position': 1, 'pageNum': 0})\n",
    "    url = 'https://www.linkedin.com/jobs/' + querystring\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_full_html(url):\n",
    "    \"\"\"Gathering the page full HTML code (w/ Selenium)\"\"\"\n",
    "    \n",
    "    print('STAGE 1: GATHERING THE PAGE FULL HTML CODE -----------------------------------------\\n')\n",
    "    \n",
    "    # chrome driver needs to be added to PATH, otherwise provide the executable path (via a Service object) \n",
    "    driver = webdriver.Chrome()\n",
    "    driver.get(url)\n",
    "\n",
    "    close = 0\n",
    "    while close == 0:\n",
    "        \n",
    "        start_while = time.time() \n",
    "    \n",
    "        # Get the number of jobs the page shows on top of the cards\n",
    "        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "        try:\n",
    "            # Click the \"Accept Cookies\" button, if it displays\n",
    "            try:\n",
    "                driver.find_element_by_xpath(\"//button[@class='artdeco-global-alert-action artdeco-button artdeco-button--inverse artdeco-button--2 artdeco-button--primary'] \\\n",
    "                                                       and @data-tracking-control-name='ga-cookie.consent.accept.v3'\") \\\n",
    "                      .click()\n",
    "                print('Cookies Accepted.\\n')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            nr_jobs = soup.find('span', class_ = 'results-context-header__job-count').text.strip()\n",
    "            print(f'Total Number of Jobs Advertised in the Top: {nr_jobs}\\n')\n",
    "\n",
    "            nr_jobs_initial = get_jobs_loaded(driver)\n",
    "            print('Number of Jobs Loaded in the Browser:')\n",
    "            print(f'  @ Opening Page: {nr_jobs_initial}')\n",
    "\n",
    "            scrolls = 0\n",
    "            buttons = 0\n",
    "\n",
    "            while soup.find('div', class_ = 'inline-notification see-more-jobs__viewed-all') is None:\n",
    "                # Stop when a \"You've viewed all jobs\" card appears\n",
    "\n",
    "                nr_jobs_loaded_init = get_jobs_loaded(driver)\n",
    "\n",
    "                try:\n",
    "                    # Click the \"Show More Jobs\" button\n",
    "                    driver.find_element_by_xpath(\"//button[@class='infinite-scroller__show-more-button infinite-scroller__show-more-button--visible']\").click()\n",
    "                    buttons += 1\n",
    "                    buttons_print = 'Button' if buttons == 1 else 'Buttons'\n",
    "\n",
    "                    # Give the browser some time to fetch the results\n",
    "                    time.sleep(1.2)\n",
    "\n",
    "                    # Printing the number of jobs already loaded\n",
    "                    nr_jobs_loaded = get_jobs_loaded(driver)\n",
    "                    if nr_jobs_loaded != nr_jobs_loaded_init:\n",
    "                        print(f'  After {buttons} {buttons_print}: {nr_jobs_loaded}')\n",
    "\n",
    "                except:\n",
    "                    \n",
    "                    # Scroll through the infinite scroll until the \"Show More Jobs\" button appears\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    scrolls += 1\n",
    "                    scrolls_print = 'Scroll' if scrolls == 1 else 'Scrolls'\n",
    "\n",
    "                    time.sleep(1.8)\n",
    "                    nr_jobs_loaded = get_jobs_loaded(driver)\n",
    "                    if nr_jobs_loaded != nr_jobs_loaded_init:\n",
    "                        print(f'  After {scrolls} {scrolls_print}: {nr_jobs_loaded}')\n",
    "                \n",
    "                finally:\n",
    "                    # Refreshing the soup for assessment in the while loop condition\n",
    "                    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "                    \n",
    "                    #If for some reason the page is taking too long to load, start again\n",
    "                    if time.time() - start_while > 100:\n",
    "                        print('Taking a while. Maybe it is better to restart.')\n",
    "                        break\n",
    "\n",
    "            # Closing the browser\n",
    "            print(\"\\nBrowser is now closed.\")\n",
    "            driver.close()\n",
    "\n",
    "            print('\\n------------------------------------------------------------------------------------\\n')\n",
    "            \n",
    "            # If we were not blocked, close the loop\n",
    "            close = 1\n",
    "            \n",
    "        except:\n",
    "            driver.close()\n",
    "            sleep_if_blocked = 30\n",
    "            print(f'Linkedin is blocking the crawling. Waiting {sleep_if_blocked} seconds to try again.')\n",
    "            time.sleep(sleep_if_blocked)\n",
    "            \n",
    "    return soup\n",
    "\n",
    "def get_jobs_loaded(driver):\n",
    "    soup_jobs = BeautifulSoup(driver.page_source)\n",
    "    nr_jobs = len(soup_jobs.find('ul', class_ = 'jobs-search__results-list').find_all('li'))\n",
    "    return nr_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gathering all information from the job cards (w/ BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_job_card_info(soup):\n",
    "    \"\"\"Gathering all information from the job cards (w/ BeautifulSoup)\"\"\"\n",
    "    \n",
    "    print('STAGE 2: GATHERING ALL INFORMATION FROM THE JOB CARDS ------------------------------\\n')\n",
    "    \n",
    "    jobs_card = soup.find('ul', class_ = 'jobs-search__results-list')\n",
    "\n",
    "    jobs = []\n",
    "    repeated_jobs = []\n",
    "    \n",
    "    for li in jobs_card.find_all('li'):\n",
    "        full_details_url = li.find('a').get('href').replace('https://pt.linkedin', 'https://linkedin')\n",
    "        position = li.find('h3', class_ = 'base-search-card__title').text.strip()\n",
    "        company = li.find('h4', class_ = 'base-search-card__subtitle').text.strip()\n",
    "        metadata = li.find('div', class_ = 'base-search-card__metadata')\n",
    "        location = metadata.find('span', class_ = 'job-search-card__location').text.strip()\n",
    "        posting_date = metadata.find('time').get('datetime')\n",
    "\n",
    "        job_info = {'Company': company,\n",
    "                    'Location': location,\n",
    "                    'Position': position,\n",
    "                    'PostingDate': posting_date,\n",
    "                    'FullDetailsURL': full_details_url[:full_details_url.find('?refId=')]}\n",
    "\n",
    "        if job_info not in jobs:\n",
    "            jobs.append(job_info)\n",
    "        else:\n",
    "            if len(repeated_jobs) == 0:\n",
    "                print('Repeated Jobs:')\n",
    "            repeated_jobs.append(job_info)\n",
    "            print(job_info['Company'], '|', job_info['Position'])\n",
    "\n",
    "    df_extr = pd.DataFrame(jobs)\n",
    "\n",
    "    print(f\"\\n{len(jobs)} unique jobs found. Full info now loaded to a dataframe.\")\n",
    "    print('\\n------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    return df_extr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gathering Full Job Info through the URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_full_info(df_extr):\n",
    "    \n",
    "    print(\"STAGE 3: GATHERING FULL JOB INFO THROUGH THE URL'S ---------------------------------\\n\")\n",
    "\n",
    "    # First instance of the dataframe \n",
    "    df_full = pd.DataFrame(columns = ['ResultsVersion', 'ResultsDate', 'Company', 'Location',\n",
    "                                      'Position', 'PostingDate', 'FullDetailsURL', 'AllQualifications', 'Applicants'])\n",
    "\n",
    "    print('Fetching results:\\n')\n",
    "    print('JobID | JobTitle | Company')\n",
    "    \n",
    "    last_version = df_full['ResultsVersion'].max() if len(df_full) > 0 else 0\n",
    "\n",
    "    for i in range(len(df_extr)):\n",
    "\n",
    "        job_info = df_extr.iloc[i].to_dict()\n",
    "        # Save the process datetime (day & hour) and a version ID\n",
    "        job_info['ResultsVersion'] = last_version + 1\n",
    "        job_info['ResultsDate'] = datetime.now().strftime(\"%d/%m/%Y %Hh\")\n",
    "\n",
    "        print(i, '|', df_extr['Position'][i], '|', df_extr['Company'][i])\n",
    "\n",
    "        job_url = df_extr['FullDetailsURL'][i]\n",
    "\n",
    "        job_page = requests.get(job_url, headers)\n",
    "        soup = BeautifulSoup(job_page.content, \"lxml\")\n",
    "\n",
    "        try:\n",
    "            # if full_description returns None, we know Linkedin blocked the request\n",
    "            full_description = soup.find('div', class_ = 'show-more-less-html__markup')\n",
    "\n",
    "            try:\n",
    "                # Store required qualifications in a list\n",
    "                qualifications = []\n",
    "                for qualification in full_description.find_all('li'):\n",
    "                    qualification = qualification.text\n",
    "                    qualifications.append(qualification)\n",
    "\n",
    "                job_info['AllQualifications'] = qualifications\n",
    "\n",
    "                try:\n",
    "                    # Job Criteria List (Employment Type, Industries, Job Function, Seniority Level)\n",
    "                    criteria = soup.find('ul', class_ = 'description__job-criteria-list')\n",
    "                    criteria_boxes = criteria.find_all('li', class_ = 'description__job-criteria-item')\n",
    "                    for box in criteria_boxes:\n",
    "                        criteria_header = box.find('h3').text.strip()\n",
    "                        criteria_text = box.find('span').text.strip()\n",
    "\n",
    "                        job_info[criteria_header] = criteria_text\n",
    "\n",
    "                    try:\n",
    "                        # Get the info regarding current applicants\n",
    "                        # If we were logged into Linkedin, we would have the exact number for those jobs under 25 applicants\n",
    "                        try:\n",
    "                            job_info['Applicants'] = soup.find('span', class_ = 'num-applicants__caption topcard__flavor--metadata topcard__flavor--bullet') \\\n",
    "                                                         .text.strip()\n",
    "                        except:\n",
    "                            #job_info['Applicants'] = soup.find('figure', class_ = 'num-applicants__figure topcard__flavor--metadata topcard__flavor--bullet') \\\n",
    "                            #                             .text.strip()\n",
    "                            job_info['Applicants'] = soup.find('figcaption', class_ = 'num-applicants__caption') \\\n",
    "                                                         .text.strip()\n",
    "                            \n",
    "                            \n",
    "                    except:\n",
    "                        print('     Errors occurred when parsing job \"Applicants\"')\n",
    "                except:\n",
    "                    print('     Errors occurred when parsing job \"Criteria\"')\n",
    "            except:\n",
    "                print('     Errors occurred when parsing job \"Qualifications\"')\n",
    "\n",
    "        except:\n",
    "            raise ValueError('LINKEDIN BLOCKED THE REQUEST')\n",
    "\n",
    "        # Add the job dict to the dataframe\n",
    "        df_full = df_full.append(job_info, ignore_index = True)\n",
    "\n",
    "        time.sleep(random.random() * 3 + 1) # Waiting a randomized amount of time (higher than 1 and lower than 4 secs)\n",
    "\n",
    "    df_full.to_csv(next_file_to_write(),\n",
    "                   index = False,\n",
    "                   encoding = 'utf-8-sig')\n",
    "    \n",
    "    print('\\n------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    return df_full\n",
    "\n",
    "\n",
    "def next_file_to_write():\n",
    "    '''Returning the next filename to write,\n",
    "    in order to refresh temporary files'''\n",
    "    \n",
    "    working_path = os.getcwd()\n",
    "\n",
    "    # Listing all csv files\n",
    "    csv_files = [file for file in os.listdir(working_path) \\\n",
    "                 if isfile(join(working_path, file)) \\\n",
    "                 and splitext(join(working_path, file))[1] == '.csv']\n",
    "\n",
    "    csv_temp_files = [file for file in csv_files if file.startswith('Run')]\n",
    "\n",
    "    # Next File to write\n",
    "    next_file = 'Run' + str(len(csv_temp_files) + 1) + '.csv'\n",
    "    \n",
    "    return next_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS -------------------------------------------------------\n",
    "# Select the company or the job you want to find results for\n",
    "keywords_in = '\"Data Scientist\"'\n",
    "# Select the location for it\n",
    "location_in = 'Lisbon'\n",
    "# --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 1\n",
    "soup = gather_full_html(url = build_url(keywords_in, location_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 2\n",
    "df_extr = gather_job_card_info(soup)\n",
    "df_extr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# STAGE 3\n",
    "df_full = gather_full_info(df_extr)\n",
    "df_full.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(\"Run Time: \" + str('%.1f' % round((end - start) / 60, 1)) + \" min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logic to update files:\n",
    "* \"01. DataExtraction\":\n",
    "  1. checks how many temporary files do we have\n",
    "  1. produces new file with index max + 1\n",
    "* \"02. DataValidation\" deletes all temporary files from the folder at the end of the day"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "vscode": {
   "interpreter": {
    "hash": "5f6577279e5a1fdffe0cd38a620c6a1cebdda05427761acda598275db198a2f2"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
