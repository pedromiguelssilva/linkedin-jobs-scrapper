{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangling & Other General Use\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# For scrapping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import urllib\n",
    "from urllib import parse\n",
    "\n",
    "# For handling files\n",
    "import os\n",
    "from os.path import isfile, join, splitext\n",
    "\n",
    "# For debugging\n",
    "from icecream import ic\n",
    "ic.configureOutput(prefix = 'Debug | ')\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gathering the page full HTML code (w/ Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_url(keywords_in, location_in):\n",
    "    \"\"\"Pass the parameters to an url parser\"\"\"\n",
    "    querystring = 'search?' + parse.urlencode({'keywords': keywords_in, 'location': location_in, 'position': 1, 'pageNum': 0})\n",
    "    url = 'https://www.linkedin.com/jobs/' + querystring\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_full_html(url):\n",
    "    \"\"\"Gathering the page full HTML code (w/ Selenium)\"\"\"\n",
    "    \n",
    "    print('STAGE 1: GATHERING THE PAGE FULL HTML CODE -----------------------------------------\\n')\n",
    "    \n",
    "    #driver_path = 'C:\\Program Files (x86)\\chromedriver.exe'\n",
    "    driver_path = 'chromedriver.exe'\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "    driver.get(url)\n",
    "\n",
    "    close = 0\n",
    "    while close == 0:\n",
    "        \n",
    "        start_while = time.time() \n",
    "    \n",
    "        # Get the number of jobs the page shows on top of the cards\n",
    "        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "        try:\n",
    "            # Click the \"Accept Cookies\" button, if it displays\n",
    "            try:\n",
    "                driver.find_element_by_xpath(\"//button[@class='artdeco-global-alert-action artdeco-button artdeco-button--inverse artdeco-button--2 artdeco-button--primary'] \\\n",
    "                                                       and @data-tracking-control-name='ga-cookie.consent.accept.v3'\") \\\n",
    "                      .click()\n",
    "                print('Cookies Accepted.\\n')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            nr_jobs = soup.find('span', class_ = 'results-context-header__job-count').text.strip()\n",
    "            print(f'Total Number of Jobs Advertised in the Top: {nr_jobs}\\n')\n",
    "\n",
    "            nr_jobs_initial = get_jobs_loaded(driver)\n",
    "            print('Number of Jobs Loaded in the Browser:')\n",
    "            print(f'  @ Opening Page: {nr_jobs_initial}')\n",
    "\n",
    "            scrolls = 0\n",
    "            buttons = 0\n",
    "\n",
    "            while soup.find('div', class_ = 'inline-notification see-more-jobs__viewed-all') is None:\n",
    "                # Stop when a \"You've viewed all jobs\" card appears\n",
    "\n",
    "                nr_jobs_loaded_init = get_jobs_loaded(driver)\n",
    "\n",
    "                try:\n",
    "                    # Click the \"Show More Jobs\" button\n",
    "                    driver.find_element_by_xpath(\"//button[@class='infinite-scroller__show-more-button infinite-scroller__show-more-button--visible']\").click()\n",
    "                    buttons += 1\n",
    "                    buttons_print = 'Button' if buttons == 1 else 'Buttons'\n",
    "\n",
    "                    # Give the browser some time to fetch the results\n",
    "                    time.sleep(1.2)\n",
    "\n",
    "                    # Printing the number of jobs already loaded\n",
    "                    nr_jobs_loaded = get_jobs_loaded(driver)\n",
    "                    if nr_jobs_loaded != nr_jobs_loaded_init:\n",
    "                        print(f'  After {buttons} {buttons_print}: {nr_jobs_loaded}')\n",
    "\n",
    "                except:\n",
    "                    \n",
    "                    # Scroll through the infinite scroll until the \"Show More Jobs\" button appears\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    scrolls += 1\n",
    "                    scrolls_print = 'Scroll' if scrolls == 1 else 'Scrolls'\n",
    "\n",
    "                    time.sleep(1.8)\n",
    "                    nr_jobs_loaded = get_jobs_loaded(driver)\n",
    "                    if nr_jobs_loaded != nr_jobs_loaded_init:\n",
    "                        print(f'  After {scrolls} {scrolls_print}: {nr_jobs_loaded}')\n",
    "                \n",
    "                finally:\n",
    "                    # Refreshing the soup for assessment in the while loop condition\n",
    "                    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "                    \n",
    "                    #If for some reason the page is taking too long to load, start again\n",
    "                    if time.time() - start_while > 100:\n",
    "                        print('Taking a while. Maybe it is better to restart.')\n",
    "                        break\n",
    "\n",
    "            # Closing the browser\n",
    "            print(\"\\nBrowser is now closed.\")\n",
    "            driver.close()\n",
    "\n",
    "            print('\\n------------------------------------------------------------------------------------\\n')\n",
    "            \n",
    "            # If we were not blocked, close the loop\n",
    "            close = 1\n",
    "            \n",
    "        except:\n",
    "            driver.close()\n",
    "            sleep_if_blocked = 30\n",
    "            print(f'Linkedin is blocking the crawling. Waiting {sleep_if_blocked} seconds to try again.')\n",
    "            time.sleep(sleep_if_blocked)\n",
    "            \n",
    "    return soup\n",
    "\n",
    "def get_jobs_loaded(driver):\n",
    "    soup_jobs = BeautifulSoup(driver.page_source)\n",
    "    nr_jobs = len(soup_jobs.find('ul', class_ = 'jobs-search__results-list').find_all('li'))\n",
    "    return nr_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gathering all information from the job cards (w/ BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_job_card_info(soup):\n",
    "    \"\"\"Gathering all information from the job cards (w/ BeautifulSoup)\"\"\"\n",
    "    \n",
    "    print('STAGE 2: GATHERING ALL INFORMATION FROM THE JOB CARDS ------------------------------\\n')\n",
    "    \n",
    "    jobs_card = soup.find('ul', class_ = 'jobs-search__results-list')\n",
    "\n",
    "    jobs = []\n",
    "    repeated_jobs = []\n",
    "    \n",
    "    for li in jobs_card.find_all('li'):\n",
    "        full_details_url = li.find('a').get('href').replace('https://pt.linkedin', 'https://linkedin')\n",
    "        position = li.find('h3', class_ = 'base-search-card__title').text.strip()\n",
    "        company = li.find('h4', class_ = 'base-search-card__subtitle').text.strip()\n",
    "        metadata = li.find('div', class_ = 'base-search-card__metadata')\n",
    "        location = metadata.find('span', class_ = 'job-search-card__location').text.strip()\n",
    "        posting_date = metadata.find('time').get('datetime')\n",
    "\n",
    "        job_info = {'Company': company,\n",
    "                    'Location': location,\n",
    "                    'Position': position,\n",
    "                    'PostingDate': posting_date,\n",
    "                    'FullDetailsURL': full_details_url[:full_details_url.find('?refId=')]}\n",
    "\n",
    "        if job_info not in jobs:\n",
    "            jobs.append(job_info)\n",
    "        else:\n",
    "            if len(repeated_jobs) == 0:\n",
    "                print('Repeated Jobs:')\n",
    "            repeated_jobs.append(job_info)\n",
    "            print(job_info['Company'], '|', job_info['Position'])\n",
    "\n",
    "    df_extr = pd.DataFrame(jobs)\n",
    "\n",
    "    print(f\"\\n{len(jobs)} unique jobs found. Full info now loaded to a dataframe.\")\n",
    "    print('\\n------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    return df_extr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gathering Full Job Info through the URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_full_info(df_extr):\n",
    "    \n",
    "    print(\"STAGE 3: GATHERING FULL JOB INFO THROUGH THE URL'S ---------------------------------\\n\")\n",
    "\n",
    "    # First instance of the dataframe \n",
    "    df_full = pd.DataFrame(columns = ['ResultsVersion', 'ResultsDate', 'Company', 'Location',\n",
    "                                      'Position', 'PostingDate', 'FullDetailsURL', 'AllQualifications', 'Applicants'])\n",
    "\n",
    "    print('Fetching results:\\n')\n",
    "    print('JobID | JobTitle | Company')\n",
    "    \n",
    "    last_version = df_full['ResultsVersion'].max() if len(df_full) > 0 else 0\n",
    "\n",
    "    for i in range(len(df_extr)):\n",
    "\n",
    "        job_info = df_extr.iloc[i].to_dict()\n",
    "        # Save the process datetime (day & hour) and a version ID\n",
    "        job_info['ResultsVersion'] = last_version + 1\n",
    "        job_info['ResultsDate'] = datetime.now().strftime(\"%d/%m/%Y %Hh\")\n",
    "\n",
    "        print(i, '|', df_extr['Position'][i], '|', df_extr['Company'][i])\n",
    "\n",
    "        job_url = df_extr['FullDetailsURL'][i]\n",
    "\n",
    "        job_page = requests.get(job_url, headers)\n",
    "        soup = BeautifulSoup(job_page.content, \"lxml\")\n",
    "\n",
    "        try:\n",
    "            # if full_description returns None, we know Linkedin blocked the request\n",
    "            full_description = soup.find('div', class_ = 'show-more-less-html__markup')\n",
    "\n",
    "            try:\n",
    "                # Store required qualifications in a list\n",
    "                qualifications = []\n",
    "                for qualification in full_description.find_all('li'):\n",
    "                    qualification = qualification.text\n",
    "                    qualifications.append(qualification)\n",
    "\n",
    "                job_info['AllQualifications'] = qualifications\n",
    "\n",
    "                try:\n",
    "                    # Job Criteria List (Employment Type, Industries, Job Function, Seniority Level)\n",
    "                    criteria = soup.find('ul', class_ = 'description__job-criteria-list')\n",
    "                    criteria_boxes = criteria.find_all('li', class_ = 'description__job-criteria-item')\n",
    "                    for box in criteria_boxes:\n",
    "                        criteria_header = box.find('h3').text.strip()\n",
    "                        criteria_text = box.find('span').text.strip()\n",
    "\n",
    "                        job_info[criteria_header] = criteria_text\n",
    "\n",
    "                    try:\n",
    "                        # Get the info regarding current applicants\n",
    "                        # If we were logged into Linkedin, we would have the exact number for those jobs under 25 applicants\n",
    "                        try:\n",
    "                            job_info['Applicants'] = soup.find('span', class_ = 'num-applicants__caption topcard__flavor--metadata topcard__flavor--bullet') \\\n",
    "                                                         .text.strip()\n",
    "                        except:\n",
    "                            #job_info['Applicants'] = soup.find('figure', class_ = 'num-applicants__figure topcard__flavor--metadata topcard__flavor--bullet') \\\n",
    "                            #                             .text.strip()\n",
    "                            job_info['Applicants'] = soup.find('figcaption', class_ = 'num-applicants__caption') \\\n",
    "                                                         .text.strip()\n",
    "                            \n",
    "                            \n",
    "                    except:\n",
    "                        print('     Errors occurred when parsing job \"Applicants\"')\n",
    "                except:\n",
    "                    print('     Errors occurred when parsing job \"Criteria\"')\n",
    "            except:\n",
    "                print('     Errors occurred when parsing job \"Qualifications\"')\n",
    "\n",
    "        except:\n",
    "            raise ValueError('LINKEDIN BLOCKED THE REQUEST')\n",
    "\n",
    "        # Add the job dict to the dataframe\n",
    "        df_full = df_full.append(job_info, ignore_index = True)\n",
    "\n",
    "        time.sleep(random.random() * 3 + 1) # Waiting a randomized amount of time (higher than 1 and lower than 4 secs)\n",
    "\n",
    "    df_full.to_csv(next_file_to_write(),\n",
    "                   index = False,\n",
    "                   encoding = 'utf-8-sig')\n",
    "    \n",
    "    print('\\n------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    return df_full\n",
    "\n",
    "\n",
    "def next_file_to_write():\n",
    "    '''Returning the next filename to write,\n",
    "    in order to refresh temporary files'''\n",
    "    \n",
    "    working_path = os.getcwd()\n",
    "\n",
    "    # Listing all csv files\n",
    "    csv_files = [file for file in os.listdir(working_path) \\\n",
    "                 if isfile(join(working_path, file)) \\\n",
    "                 and splitext(join(working_path, file))[1] == '.csv']\n",
    "\n",
    "    csv_temp_files = [file for file in csv_files if file.startswith('Run')]\n",
    "\n",
    "    # Next File to write\n",
    "    next_file = 'Run' + str(len(csv_temp_files) + 1) + '.csv'\n",
    "    \n",
    "    return next_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS -------------------------------------------------------\n",
    "# Select the company or the job you want to find results for\n",
    "keywords_in = '\"Data Scientist\"'\n",
    "# Select the location for it\n",
    "location_in = 'Lisbon'\n",
    "# --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 1: GATHERING THE PAGE FULL HTML CODE -----------------------------------------\n",
      "\n",
      "Total Number of Jobs Advertised in the Top: 78\n",
      "\n",
      "Number of Jobs Loaded in the Browser:\n",
      "  @ Opening Page: 25\n",
      "  After 1 Scroll: 50\n",
      "  After 2 Scrolls: 75\n",
      "  After 3 Scrolls: 78\n",
      "\n",
      "Browser is now closed.\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STAGE 1\n",
    "soup = gather_full_html(build_url(keywords_in, location_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 2: GATHERING ALL INFORMATION FROM THE JOB CARDS ------------------------------\n",
      "\n",
      "\n",
      "78 unique jobs found. Full info now loaded to a dataframe.\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Position</th>\n",
       "      <th>PostingDate</th>\n",
       "      <th>FullDetailsURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>McKinsey &amp; Company</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-09-04</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Siemens</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist (m/f/d)</td>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siemens</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Junior Data Scientist (m/f/d)</td>\n",
       "      <td>2021-08-01</td>\n",
       "      <td>https://linkedin.com/jobs/view/junior-data-sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Capgemini Engineering</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist (M/F) - Lisbon</td>\n",
       "      <td>2021-09-06</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Nokia</td>\n",
       "      <td>Amadora, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist (Traineeship)</td>\n",
       "      <td>2021-07-13</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                 Company                   Location  \\\n",
       "0     McKinsey & Company   Lisbon, Lisbon, Portugal   \n",
       "1                Siemens   Lisbon, Lisbon, Portugal   \n",
       "2                Siemens   Lisbon, Lisbon, Portugal   \n",
       "3  Capgemini Engineering   Lisbon, Lisbon, Portugal   \n",
       "4                  Nokia  Amadora, Lisbon, Portugal   \n",
       "\n",
       "                        Position PostingDate  \\\n",
       "0                 Data Scientist  2021-09-04   \n",
       "1         Data Scientist (m/f/d)  2021-08-08   \n",
       "2  Junior Data Scientist (m/f/d)  2021-08-01   \n",
       "3  Data Scientist (M/F) - Lisbon  2021-09-06   \n",
       "4   Data Scientist (Traineeship)  2021-07-13   \n",
       "\n",
       "                                      FullDetailsURL  \n",
       "0  https://linkedin.com/jobs/view/data-scientist-...  \n",
       "1  https://linkedin.com/jobs/view/data-scientist-...  \n",
       "2  https://linkedin.com/jobs/view/junior-data-sci...  \n",
       "3  https://linkedin.com/jobs/view/data-scientist-...  \n",
       "4  https://linkedin.com/jobs/view/data-scientist-...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STAGE 2\n",
    "df_extr = gather_job_card_info(soup)\n",
    "df_extr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 3: GATHERING FULL JOB INFO THROUGH THE URL'S ---------------------------------\n",
      "\n",
      "Fetching results:\n",
      "\n",
      "JobID | JobTitle | Company\n",
      "0 | Data Scientist | McKinsey & Company\n",
      "1 | Data Scientist (m/f/d) | Siemens\n",
      "2 | Junior Data Scientist (m/f/d) | Siemens\n",
      "3 | Data Scientist (M/F) - Lisbon | Capgemini Engineering\n",
      "4 | Data Scientist (Traineeship) | Nokia\n",
      "5 | Data Scientist, Portugal | CI&T\n",
      "6 | Data Scientist | SGS\n",
      "7 | Data Scientist | Worldpanel by Kantar\n",
      "8 | Data Scientist | Mind Source\n",
      "9 | Data Scientist | Feedzai\n",
      "10 | Senior Data Scientist | Tripadvisor\n",
      "11 | Product Data Scientist | CASAFARI\n",
      "12 | Data Scientist | Winning\n",
      "13 | Senior Data Scientist | YData\n",
      "14 | Data Scientist (m/f) | Smart Consulting\n",
      "15 | Data Scientist (m/f) | Findmore\n",
      "16 | Data Scientist | Findmore\n",
      "17 | Data Scientist - m/f | Michael Page\n",
      "18 | Senior Data Scientist | Beats Medical\n",
      "19 | Data Scientist (m/f) | ADENTIS\n",
      "20 | Data Scientist | BOLD by Devoteam\n",
      "21 | Data Scientist | BOLD by Devoteam\n",
      "22 | Data Scientist | BOLD by Devoteam\n",
      "23 | Data Scientist | Axianspt\n",
      "24 | Data Scientist (PhD) | CGI\n",
      "25 | Data Scientist | BOLD by Devoteam\n",
      "26 | Data Scientist | BOLD by Devoteam\n",
      "27 | Data scientist | BOLD by Devoteam\n",
      "28 | Senior Data Scientist | Nokia\n",
      "29 | Data Scientist (m/f) ? Lisboa | Ankix\n",
      "30 | Data Scientist | Axians Portugal\n",
      "31 | Data Scientist (A) [621] | Axians Portugal\n",
      "32 | Data Scientist [A] (356) | Axians Portugal\n",
      "33 | Data Scientist – AI & ML (M/F) Lisboa | Adecco Specialized Recruitment\n",
      "34 | Data Scientist [A] (356) | Axianspt\n",
      "35 | Quantitative Research - Data Scientist | BNP Paribas CIB\n",
      "36 | Data Scientist (A) [621] | Axianspt\n",
      "37 | Sr. Data Scientist - Bot Management | Cloudflare\n",
      "38 | Data Scientist (m/f) | Bee Engineering ICT\n",
      "39 | Senior Data Scientist | Faber Talent Pool\n",
      "40 | Data Scientist | BOLD by Devoteam\n",
      "41 | Data Scientist (B) [604] | Axians Portugal\n",
      "42 | Data Scientist (B) [604] | Axianspt\n",
      "43 | Data Scientist - Customer Analytics | Networkers - Technology Recruitment\n",
      "44 | SMART DATA SCIENTIST | Lisboa | Smart Consulting\n",
      "45 | Data Scientist | BOLD by Devoteam\n",
      "46 | Data Manager/Data Scientist (M/F) | JLL\n",
      "47 | Oferta de emprego: Data Scientist | Boost IT\n",
      "48 | Data Scientist em NLP | INOV ? Instituto de Engenharia de Sistemas e Computadores Inovação\n",
      "49 | Data Scientist - Machine Learning & NLP | INOV ? Instituto de Engenharia de Sistemas e Computadores Inovação\n",
      "50 | Oferta de trabalho Data Scientist (M/F) | Habber Tec\n",
      "51 | Anúncio de emprego: Data Scientist | Boost IT\n",
      "52 | Data Science & NLP | INOV ? Instituto de Engenharia de Sistemas e Computadores Inovação\n",
      "53 | Anúncio de emprego: Data Scientist (M/F) | askblue\n",
      "54 | Data Scientist (Nearshore Project in Lisbon) | BOLD by Devoteam\n",
      "55 | Oferta de emprego: Data Scientist (m/f) – Lisboa | Ankix\n",
      "56 | Oferta de emprego: Data Scientist | Vantis - Tecnologias de Informação, Lda\n",
      "57 | Oferta de trabalho Data Scientist (SAS E-Miner) (F/M) | Habber Tec\n",
      "58 | Anúncio de emprego: Senior Data Scientist | zytics\n",
      "59 | Anúncio de emprego: Data Scientist (SAS E-Miner) (F/M) | Habber Tec\n",
      "60 | Senior Data Scientist / Engineer - Stress Testing and Data Analytics | BNP Paribas CIB\n",
      "61 | Oferta de emprego: Data scientist for International Institution | KCS iT\n",
      "62 | Anúncio de emprego: Data Scientist(S) | KCS iT\n",
      "63 | Anúncio de emprego: Senior Data Scientist (m/f) – Lisboa | Ankix\n",
      "64 | Senior Data Scientist | Toptal\n",
      "65 | Senior Machine Learning | BNP Paribas CIB\n",
      "66 | Consultant, Analytics, Data and Services | Mastercard\n",
      "67 | Senior Python Data Engineer | Daltix\n",
      "68 | Data Engineer - m/f | Michael Page\n",
      "69 | BI Consultant | Reloading\n",
      "70 | BI Consultant (m/f) | Reloading\n",
      "71 | Recruiter - Digital & Analytics | McKinsey & Company\n",
      "72 | Epidemiologist | IQVIA\n",
      "73 | Model Risk Manager - Portugal Remote | Revolut\n",
      "74 | Senior Product Designer - AI | Talkdesk\n",
      "75 | Senior Data Engineer | ConvaTec\n",
      "76 | Capacity Planning Program Manager | Cloudflare\n",
      "77 | Procurement Intelligence SAP BW Specialist | Nokia\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ResultsVersion</th>\n",
       "      <th>ResultsDate</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Position</th>\n",
       "      <th>PostingDate</th>\n",
       "      <th>FullDetailsURL</th>\n",
       "      <th>AllQualifications</th>\n",
       "      <th>Applicants</th>\n",
       "      <th>Employment type</th>\n",
       "      <th>Industries</th>\n",
       "      <th>Job function</th>\n",
       "      <th>Seniority level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>06/09/2021 14h</td>\n",
       "      <td>McKinsey &amp; Company</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-09-04</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "      <td>[Master’s degree in a quantitative field like ...</td>\n",
       "      <td>Be among the first 25 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Automotive, Aviation &amp; Aerospace, and Manageme...</td>\n",
       "      <td>Consulting, Information Technology, and Marketing</td>\n",
       "      <td>Associate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>06/09/2021 14h</td>\n",
       "      <td>Siemens</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist (m/f/d)</td>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "      <td>[Work with large, complex data sets and applyi...</td>\n",
       "      <td>Be among the first 25 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Electrical/Electronic Manufacturing</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>06/09/2021 14h</td>\n",
       "      <td>Siemens</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Junior Data Scientist (m/f/d)</td>\n",
       "      <td>2021-08-01</td>\n",
       "      <td>https://linkedin.com/jobs/view/junior-data-sci...</td>\n",
       "      <td>[Build predictive and machine learning models ...</td>\n",
       "      <td>Be among the first 25 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Electrical/Electronic Manufacturing</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Associate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "  ResultsVersion     ResultsDate             Company  \\\n",
       "0              1  06/09/2021 14h  McKinsey & Company   \n",
       "1              1  06/09/2021 14h             Siemens   \n",
       "2              1  06/09/2021 14h             Siemens   \n",
       "\n",
       "                   Location                       Position PostingDate  \\\n",
       "0  Lisbon, Lisbon, Portugal                 Data Scientist  2021-09-04   \n",
       "1  Lisbon, Lisbon, Portugal         Data Scientist (m/f/d)  2021-08-08   \n",
       "2  Lisbon, Lisbon, Portugal  Junior Data Scientist (m/f/d)  2021-08-01   \n",
       "\n",
       "                                      FullDetailsURL  \\\n",
       "0  https://linkedin.com/jobs/view/data-scientist-...   \n",
       "1  https://linkedin.com/jobs/view/data-scientist-...   \n",
       "2  https://linkedin.com/jobs/view/junior-data-sci...   \n",
       "\n",
       "                                   AllQualifications  \\\n",
       "0  [Master’s degree in a quantitative field like ...   \n",
       "1  [Work with large, complex data sets and applyi...   \n",
       "2  [Build predictive and machine learning models ...   \n",
       "\n",
       "                         Applicants Employment type  \\\n",
       "0  Be among the first 25 applicants       Full-time   \n",
       "1  Be among the first 25 applicants       Full-time   \n",
       "2  Be among the first 25 applicants       Full-time   \n",
       "\n",
       "                                          Industries  \\\n",
       "0  Automotive, Aviation & Aerospace, and Manageme...   \n",
       "1                Electrical/Electronic Manufacturing   \n",
       "2                Electrical/Electronic Manufacturing   \n",
       "\n",
       "                                        Job function   Seniority level  \n",
       "0  Consulting, Information Technology, and Marketing         Associate  \n",
       "1                             Information Technology  Mid-Senior level  \n",
       "2                             Information Technology         Associate  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STAGE 3\n",
    "df_full = gather_full_info(df_extr)\n",
    "df_full.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time: 5.0 min\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Run Time: \" + str('%.1f' % round((end - start) / 60, 1)) + \" min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logic to update files:\n",
    "* \"01. DataExtraction\":\n",
    "  1. checks how many temporary files do we have\n",
    "  1. produces new file with index max + 1\n",
    "* \"02. DataValidation\" deletes all temporary files from the folder at the end of the day"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
