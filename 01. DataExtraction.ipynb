{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangling & Other General Use\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# For scrapping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import urllib\n",
    "from urllib import parse\n",
    "\n",
    "# For handling files\n",
    "import os\n",
    "from os.path import isfile, join, splitext\n",
    "\n",
    "# For debugging\n",
    "from icecream import ic\n",
    "ic.configureOutput(prefix = 'Debug | ')\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gathering the page full HTML code (w/ Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_url(keywords_in, location_in):\n",
    "    \"\"\"Pass the parameters to an url parser\"\"\"\n",
    "    querystring = 'search?' + parse.urlencode({'keywords': keywords_in, 'location': location_in, 'position': 1, 'pageNum': 0})\n",
    "    url = 'https://www.linkedin.com/jobs/' + querystring\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_full_html(url):\n",
    "    \"\"\"Gathering the page full HTML code (w/ Selenium)\"\"\"\n",
    "    \n",
    "    print('STAGE 1: Gathering the page full HTML code -----------------------------------------\\n')\n",
    "    \n",
    "    #driver_path = 'C:\\Program Files (x86)\\chromedriver.exe'\n",
    "    driver_path = 'chromedriver.exe'\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "    driver.get(url)\n",
    "\n",
    "    close = 0\n",
    "    while close == 0:\n",
    "        \n",
    "        start_while = time.time() \n",
    "    \n",
    "        # Get the number of jobs the page shows on top of the cards\n",
    "        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "        try:\n",
    "            # Click the \"Accept Cookies\" button, if it displays\n",
    "            try:\n",
    "                driver.find_element_by_xpath(\"//button[@class='artdeco-global-alert-action artdeco-button artdeco-button--inverse artdeco-button--2 artdeco-button--primary'] \\\n",
    "                                                       and @data-tracking-control-name='ga-cookie.consent.accept.v3'\") \\\n",
    "                      .click()\n",
    "                print('Cookies Accepted.\\n')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            nr_jobs = soup.find('span', class_ = 'results-context-header__job-count').text.strip()\n",
    "            print(f'Total Number of Jobs Advertised in the Top: {nr_jobs}\\n')\n",
    "\n",
    "            nr_jobs_initial = get_jobs_loaded(driver)\n",
    "            print('Number of Jobs Loaded in the Browser:')\n",
    "            print(f'  @ Opening Page: {nr_jobs_initial}')\n",
    "\n",
    "            scrolls = 0\n",
    "            buttons = 0\n",
    "\n",
    "            while soup.find('div', class_ = 'inline-notification see-more-jobs__viewed-all') is None:\n",
    "                # Stop when a \"You've viewed all jobs\" card appears\n",
    "\n",
    "                nr_jobs_loaded_init = get_jobs_loaded(driver)\n",
    "\n",
    "                try:\n",
    "                    # Click the \"Show More Jobs\" button\n",
    "                    driver.find_element_by_xpath(\"//button[@class='infinite-scroller__show-more-button infinite-scroller__show-more-button--visible']\").click()\n",
    "                    buttons += 1\n",
    "                    buttons_print = 'Button' if buttons == 1 else 'Buttons'\n",
    "\n",
    "                    # Give the browser some time to fetch the results\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    # Printing the number of jobs already loaded\n",
    "                    nr_jobs_loaded = get_jobs_loaded(driver)\n",
    "                    if nr_jobs_loaded != nr_jobs_loaded_init:\n",
    "                        print(f'  After {buttons} {buttons_print}: {nr_jobs_loaded}')\n",
    "\n",
    "                except:\n",
    "                    \n",
    "                    # Scroll through the infinite scroll until the \"Show More Jobs\" button appears\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    scrolls += 1\n",
    "                    scrolls_print = 'Scroll' if scrolls == 1 else 'Scrolls'\n",
    "\n",
    "                    time.sleep(1.2)\n",
    "                    nr_jobs_loaded = get_jobs_loaded(driver)\n",
    "                    if nr_jobs_loaded != nr_jobs_loaded_init:\n",
    "                        print(f'  After {scrolls} {scrolls_print}: {nr_jobs_loaded}')\n",
    "                \n",
    "                finally:\n",
    "                    # Refreshing the soup for assessment in the while loop condition\n",
    "                    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "                    \n",
    "                    #If for some reason the page is taking too long to load, start again\n",
    "                    if time.time() - start_while > 100:\n",
    "                        print('Taking a while. Maybe it is better to restart.')\n",
    "                        break\n",
    "\n",
    "            # Closing the browser\n",
    "            print(\"\\nBrowser is now closed.\")\n",
    "            driver.close()\n",
    "\n",
    "            print('\\n------------------------------------------------------------------------------------\\n')\n",
    "            \n",
    "            # If we were not blocked, close the loop\n",
    "            close = 1\n",
    "            \n",
    "        except:\n",
    "            driver.close()\n",
    "            sleep_if_blocked = 30\n",
    "            print(f'Linkedin is blocking the crawling. Waiting {sleep_if_blocked} seconds to try again.')\n",
    "            time.sleep(sleep_if_blocked)\n",
    "            \n",
    "    return soup\n",
    "\n",
    "def get_jobs_loaded(driver):\n",
    "    soup_jobs = BeautifulSoup(driver.page_source)\n",
    "    nr_jobs = len(soup_jobs.find('ul', class_ = 'jobs-search__results-list').find_all('li'))\n",
    "    return nr_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gathering all information from the job cards (w/ BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_job_card_info(soup):\n",
    "    \"\"\"Gathering all information from the job cards (w/ BeautifulSoup)\"\"\"\n",
    "    \n",
    "    print('STAGE 2: Gathering all information from the job cards ------------------------------\\n')\n",
    "    \n",
    "    jobs_card = soup.find('ul', class_ = 'jobs-search__results-list')\n",
    "\n",
    "    jobs = []\n",
    "    repeated_jobs = []\n",
    "    \n",
    "    for li in jobs_card.find_all('li'):\n",
    "        full_details_url = li.find('a').get('href').replace('https://pt.linkedin', 'https://linkedin')\n",
    "        position = li.find('h3', class_ = 'base-search-card__title').text.strip()\n",
    "        company = li.find('h4', class_ = 'base-search-card__subtitle').text.strip()\n",
    "        metadata = li.find('div', class_ = 'base-search-card__metadata')\n",
    "        location = metadata.find('span', class_ = 'job-search-card__location').text.strip()\n",
    "        posting_date = metadata.find('time').get('datetime')\n",
    "\n",
    "        job_info = {'Company': company,\n",
    "                    'Location': location,\n",
    "                    'Position': position,\n",
    "                    'PostingDate': posting_date,\n",
    "                    'FullDetailsURL': full_details_url[:full_details_url.find('?refId=')]}\n",
    "\n",
    "        if job_info not in jobs:\n",
    "            jobs.append(job_info)\n",
    "        else:\n",
    "            if len(repeated_jobs) == 0:\n",
    "                print('Repeated Jobs:')\n",
    "            repeated_jobs.append(job_info)\n",
    "            print(job_info['Company'], '|', job_info['Position'])\n",
    "\n",
    "    df_extr = pd.DataFrame(jobs)\n",
    "\n",
    "    print(f\"\\n{len(jobs)} unique jobs found. Full info now loaded to a dataframe.\")\n",
    "    print('\\n------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    return df_extr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gathering Full Job Info through the URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_full_info(df_extr):\n",
    "    \n",
    "    print(\"STAGE 3: Gathering Full Job Info through the URL's ---------------------------------\\n\")\n",
    "\n",
    "    try:\n",
    "        # Reading previous days info from csv file\n",
    "        df_full = pd.read_csv('FullInfoDataframe.csv') \n",
    "    except:\n",
    "        # First instance of the dataframe \n",
    "        df_full = pd.DataFrame(columns = ['ResultsVersion', 'ResultsDate', 'Company', 'Location',\n",
    "                                          'Position', 'PostingDate', 'FullDetailsURL', 'AllQualifications', 'Applicants'])\n",
    "        df_full.to_csv('FullInfoDataframe.csv',\n",
    "                       index = False,\n",
    "                       encoding = 'utf-8-sig')\n",
    "\n",
    "    print('Fetching results:\\n')\n",
    "    print('JobID | JobTitle | Company')\n",
    "    \n",
    "    last_version = df_full['ResultsVersion'].max() if len(df_full) > 0 else 0\n",
    "\n",
    "    for i in range(len(df_extr)):\n",
    "\n",
    "        job_info = df_extr.iloc[i].to_dict()\n",
    "        # Save the process datetime (day & hour) and a version ID\n",
    "        job_info['ResultsVersion'] = last_version + 1\n",
    "        job_info['ResultsDate'] = datetime.now().strftime(\"%d/%m/%Y %Hh\")\n",
    "\n",
    "        print(i, '|', df_extr['Position'][i], '|', df_extr['Company'][i])\n",
    "\n",
    "        job_url = df_extr['FullDetailsURL'][i]\n",
    "\n",
    "        job_page = requests.get(job_url, headers)\n",
    "        soup = BeautifulSoup(job_page.content, \"lxml\")\n",
    "\n",
    "        try:\n",
    "            # if full_description returns None, we know Linkedin blocked the request\n",
    "            full_description = soup.find('div', class_ = 'show-more-less-html__markup')\n",
    "\n",
    "            try:\n",
    "                # Store required qualifications in a list\n",
    "                qualifications = []\n",
    "                for qualification in full_description.find_all('li'):\n",
    "                    qualification = qualification.text\n",
    "                    qualifications.append(qualification)\n",
    "\n",
    "                job_info['AllQualifications'] = qualifications\n",
    "\n",
    "                try:\n",
    "                    # Job Criteria List (Employment Type, Industries, Job Function, Seniority Level)\n",
    "                    criteria = soup.find('ul', class_ = 'description__job-criteria-list')\n",
    "                    criteria_boxes = criteria.find_all('li', class_ = 'description__job-criteria-item')\n",
    "                    for box in criteria_boxes:\n",
    "                        criteria_header = box.find('h3').text.strip()\n",
    "                        criteria_text = box.find('span').text.strip()\n",
    "\n",
    "                        job_info[criteria_header] = criteria_text\n",
    "\n",
    "                    try:\n",
    "                        # Get the info regarding current applicants\n",
    "                        # If we were logged into Linkedin, we would have the exact number for those jobs under 25 applicants\n",
    "                        try:\n",
    "                            job_info['Applicants'] = soup.find('span', class_ = 'num-applicants__caption topcard__flavor--metadata topcard__flavor--bullet') \\\n",
    "                                                         .text.strip()\n",
    "                        except:\n",
    "                            #job_info['Applicants'] = soup.find('figure', class_ = 'num-applicants__figure topcard__flavor--metadata topcard__flavor--bullet') \\\n",
    "                            #                             .text.strip()\n",
    "                            job_info['Applicants'] = soup.find('figcaption', class_ = 'num-applicants__caption') \\\n",
    "                                                         .text.strip()\n",
    "                            \n",
    "                            \n",
    "                    except:\n",
    "                        print('     Errors occurred when parsing job \"Applicants\"')\n",
    "                except:\n",
    "                    print('     Errors occurred when parsing job \"Criteria\"')\n",
    "            except:\n",
    "                print('     Errors occurred when parsing job \"Qualifications\"')\n",
    "\n",
    "        except:\n",
    "            raise ValueError('LINKEDIN BLOCKED THE REQUEST')\n",
    "\n",
    "        # Add the job dict to the dataframe\n",
    "        df_full = df_full.append(job_info, ignore_index = True)\n",
    "\n",
    "        time.sleep(random.random() * 3 + 1) # Waiting a randomized amount of time (higher than 1 and lower than 4 secs)\n",
    "\n",
    "    df_full.to_csv(next_file_to_write(),\n",
    "                   index = False,\n",
    "                   encoding = 'utf-8-sig')\n",
    "    \n",
    "    print('\\n------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 4. Refreshing temporary files"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def next_file_to_write():\n",
    "\n",
    "    working_path = os.getcwd()\n",
    "\n",
    "    # Listing all csv files\n",
    "    csv_files = [file for file in os.listdir(working_path) \\\n",
    "                 if isfile(join(working_path, file)) \\\n",
    "                 and splitext(join(working_path, file))[1] == '.csv']\n",
    "\n",
    "    csv_temp_files = [file for file in csv_files if file.startswith('Run')]\n",
    "\n",
    "    # Next File to write\n",
    "    next_file = 'Run' + str(len(csv_temp_files) + 1) + '.csv'\n",
    "    \n",
    "    return next_file"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS -------------------------------------------------------\n",
    "# Select the company or the job you want to find results for\n",
    "keywords_in = '\"Data Scientist\"'\n",
    "# Select the location for it\n",
    "location_in = 'Lisbon'\n",
    "# --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 1: Gathering the page full HTML code -----------------------------------------\n",
      "\n",
      "Total Number of Jobs Advertised in the Top: 97\n",
      "\n",
      "Number of Jobs Loaded in the Browser:\n",
      "  @ Opening Page: 25\n",
      "  After 3 Scrolls: 75\n",
      "  After 4 Scrolls: 97\n",
      "\n",
      "Browser is now closed.\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STAGE 1\n",
    "soup = gather_full_html(build_url(keywords_in, location_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 2: Gathering all information from the job cards ------------------------------\n",
      "\n",
      "\n",
      "97 unique jobs found. Full info now loaded to a dataframe.\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Position</th>\n",
       "      <th>PostingDate</th>\n",
       "      <th>FullDetailsURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>Celfinet</td>\n",
       "      <td>Lisbon, Portugal</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-08-09</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>McKinsey &amp; Company</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-08-08</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Tripadvisor</td>\n",
       "      <td>Lisboa, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-08-04</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>CASAFARI</td>\n",
       "      <td>Lisbon, Portugal</td>\n",
       "      <td>Product Data Scientist</td>\n",
       "      <td>2021-07-16</td>\n",
       "      <td>https://linkedin.com/jobs/view/product-data-sc...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>SGS</td>\n",
       "      <td>Lisboa, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-07-19</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Company                  Location                Position  \\\n",
       "0            Celfinet          Lisbon, Portugal          Data Scientist   \n",
       "1  McKinsey & Company  Lisbon, Lisbon, Portugal          Data Scientist   \n",
       "2         Tripadvisor  Lisboa, Lisbon, Portugal          Data Scientist   \n",
       "3            CASAFARI          Lisbon, Portugal  Product Data Scientist   \n",
       "4                 SGS  Lisboa, Lisbon, Portugal          Data Scientist   \n",
       "\n",
       "  PostingDate                                     FullDetailsURL  \n",
       "0  2021-08-09  https://linkedin.com/jobs/view/data-scientist-...  \n",
       "1  2021-08-08  https://linkedin.com/jobs/view/data-scientist-...  \n",
       "2  2021-08-04  https://linkedin.com/jobs/view/data-scientist-...  \n",
       "3  2021-07-16  https://linkedin.com/jobs/view/product-data-sc...  \n",
       "4  2021-07-19  https://linkedin.com/jobs/view/data-scientist-...  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STAGE 2\n",
    "df_extr = gather_job_card_info(soup)\n",
    "df_extr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 3: Gathering Full Job Info through the URL's ---------------------------------\n",
      "\n",
      "Fetching results:\n",
      "\n",
      "JobID | JobTitle | Company\n",
      "0 | Data Scientist | Celfinet\n",
      "1 | Data Scientist | McKinsey & Company\n",
      "2 | Data Scientist | Tripadvisor\n",
      "3 | Product Data Scientist | CASAFARI\n",
      "4 | Data Scientist | SGS\n",
      "5 | Data Scientist | Tripadvisor\n",
      "6 | Data Scientist (m/f/d) | Siemens\n",
      "7 | Data Scientist | Feedzai\n",
      "8 | Junior Data Scientist (m/f/d) | Siemens\n",
      "9 | Data Scientist | BOLD by Devoteam\n",
      "10 | Data Scientist (Traineeship) | Nokia\n",
      "11 | Data scientist | BOLD by Devoteam\n",
      "12 | Data Scientist | BOLD by Devoteam\n",
      "13 | Data Scientist (M/F) - Lisbon | Capgemini Engineering\n",
      "14 | Senior Data Scientist (Lisbon) | PandaDoc\n",
      "15 | Senior Data Scientist | Tripadvisor\n",
      "16 | Data Scientist | everis\n",
      "17 | Junior Data Scientist – Scoring Centre (M/F) | Wyser\n",
      "18 | Data Scientist | BOLD by Devoteam\n",
      "19 | Data Scientist | Amgen\n",
      "20 | Data Scientist | Solvay\n",
      "21 | Data Scientist | BOLD by Devoteam\n",
      "22 | Quantitative Research - Data Scientist | BNP Paribas CIB\n",
      "23 | Data Scientist | BOLD by Devoteam\n",
      "24 | Senior Data Scientist (all genders) | 4TE\n",
      "25 | Data Scientist Pricing | Zurich Insurance Company Ltd\n",
      "26 | Data Scientist Junior (SQL+R) | VASP - Distribuidora de Publicações, S.A.\n",
      "27 | Sr. Data Scientist - Bot Management | Cloudflare\n",
      "28 | Data Scientist - Lisboa ou Porto | NOS SGPS\n",
      "29 | Data Scientist | Axians Portugal\n",
      "30 | Data Scientist - m/f | Michael Page\n",
      "31 | Senior Data Scientist | Faber Talent Pool\n",
      "32 | Data Scientist | Findmore\n",
      "33 | Data Manager/Data Scientist (M/F) | JLL\n",
      "34 | Data Scientist – AI & ML (M/F) Lisboa | Adecco Specialized Recruitment\n",
      "35 | Senior Data Scientist – Scoring Centre (M/F) | Wyser\n",
      "36 | Machine Learning Specialist | Experis\n",
      "37 | Data Scientist (m/f) | Noesis\n",
      "38 | Data Scientist (m/f) | Bee Engineering ICT\n",
      "39 | Data Scientist (m/f) | Findmore\n",
      "40 | Data Science & NLP | INOV ? Instituto de Engenharia de Sistemas e Computadores Inovação\n",
      "41 | Data Scientist (m/f) | IT People Innovation\n",
      "42 | Senior Data Scientist | KDR Recruitment Ltd\n",
      "43 | Data Scientist (m/f) | Mind Source\n",
      "44 | Data Manager/Data Scientist (M/F) | JLL\n",
      "45 | Data Scientist em NLP | INOV ? Instituto de Engenharia de Sistemas e Computadores Inovação\n",
      "46 | Data Scientist - Customer Analytics | Networkers - Technology Recruitment\n",
      "47 | Data Scientist - Machine Learning & NLP | INOV ? Instituto de Engenharia de Sistemas e Computadores Inovação\n",
      "48 | Data Scientist (B) [604] | Axians\n",
      "49 | Data Scientist (A) [621] | Axians\n",
      "50 | Lead Consultant, Analytics, Data and Services | Mastercard\n",
      "51 | Senior Data Scientist (M/F) | Timing Recursos Humanos\n",
      "52 | Data Scientist [A] (356) | Axians Portugal\n",
      "53 | SMART DATA SCIENTIST | Lisboa | Smart Consulting\n",
      "54 | Senior Data Scientist | Findmore\n",
      "55 | Data Scientist (m/f) ? Lisboa | Ankix\n",
      "56 | Data Scientist (Nearshore Project in Lisbon) | BOLD by Devoteam\n",
      "57 | Senior Data Scientist Consultant & Data Lover | Keyrus\n",
      "58 | Data Scientist (B) [604] | Axians Portugal\n",
      "59 | Consultant, Analytics, Data and Services | Mastercard\n",
      "60 | Senior Data Scientist (m/f) | Findmore\n",
      "61 | Data Scientist (A) [621] | Axians Portugal\n",
      "62 | Data Scientist (French) (m/f) | Integer Consulting\n",
      "63 | Data Scientist (A) [621] | Axianspt\n",
      "64 | Data Scientist (B) [604] | Axianspt\n",
      "65 | Anúncio de emprego: Data Scientist | Boost IT\n",
      "66 | Oferta de trabalho Data Scientist (M/F) | Habber Tec\n",
      "67 | Oferta de emprego: Data Scientist | Boost IT\n",
      "68 | Senior Data Scientist / Engineer - Stress Testing and Data Analytics | BNP Paribas CIB\n",
      "69 | Oferta de emprego: Data scientist for International Institution | KCS iT\n",
      "70 | Anúncio de emprego: Data Scientist (M/F) | askblue\n",
      "71 | Oferta de emprego: Data Scientist (m/f) – Lisboa | Ankix\n",
      "72 | Oferta de trabalho Data Scientist (SAS E-Miner) (F/M) | Habber Tec\n",
      "73 | Data Scientist – Remote, Full-time | Toptal\n",
      "74 | Data Scientist – Remote, Full-time | Toptal\n",
      "75 | Anúncio de emprego: Data Scientist(S) | KCS iT\n",
      "76 | Senior Machine Learning | BNP Paribas CIB\n",
      "77 | Anúncio de emprego: Senior Data Scientist | zytics\n",
      "78 | Epidemiologist | IQVIA\n",
      "79 | Anúncio de emprego: Data Scientist (SAS E-Miner) (F/M) | Habber Tec\n",
      "80 | Oferta de emprego: Data Scientist | Vantis - Tecnologias de Informação, Lda\n",
      "81 | BI Consultant | Reloading\n",
      "82 | Senior Python Data Engineer | Daltix\n",
      "83 | Anúncio de emprego: Senior Data Scientist (m/f) – Lisboa | Ankix\n",
      "84 | Oferta de emprego: Consultor Data Scientist (m/f) | Match Profiler\n",
      "85 | Real World Data Senior Biostatistician | IQVIA\n",
      "86 | Data Scientist Intern (m/f/d) | Siemens\n",
      "87 | Anúncio de emprego: Data Science- Lisboa | Fyld\n",
      "88 | Model Risk Manager | Revolut\n",
      "89 | BI Consultant (m/f) | Reloading\n",
      "90 | Capacity Planning Analyst | Cloudflare\n",
      "91 | Senior Data Engineer | ConvaTec\n",
      "92 | Data Engineer - m/f | Michael Page\n",
      "93 | Senior Product Designer - AI | Talkdesk\n",
      "94 | Capacity Planning Engineer | Cloudflare\n",
      "95 | Capacity Planning Program Manager | Cloudflare\n",
      "96 | Anúncio de emprego: Machine Learning Engineer (m/f) - Nearshore | Passio Consulting\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ResultsVersion</th>\n",
       "      <th>ResultsDate</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Position</th>\n",
       "      <th>PostingDate</th>\n",
       "      <th>FullDetailsURL</th>\n",
       "      <th>AllQualifications</th>\n",
       "      <th>Applicants</th>\n",
       "      <th>Employment type</th>\n",
       "      <th>Industries</th>\n",
       "      <th>Job function</th>\n",
       "      <th>Seniority level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>30/07/2021 14h</td>\n",
       "      <td>McKinsey &amp; Company</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-07-17</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "      <td>['Master’s degree in a quantitative field like...</td>\n",
       "      <td>41 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Automotive, Aviation &amp; Aerospace, and Manageme...</td>\n",
       "      <td>Consulting, Information Technology, and Marketing</td>\n",
       "      <td>Associate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>30/07/2021 14h</td>\n",
       "      <td>Siemens</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist (m/f/d)</td>\n",
       "      <td>2021-07-03</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "      <td>['Work with large, complex data sets and apply...</td>\n",
       "      <td>Be among the first 25 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Electrical/Electronic Manufacturing</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>30/07/2021 14h</td>\n",
       "      <td>BOLD by Devoteam</td>\n",
       "      <td>Lisbon, Portugal</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-07-30</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "      <td>['Bachelor’s degree in the IT area or equivale...</td>\n",
       "      <td>29 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Information Technology and Services</td>\n",
       "      <td>Engineering and Information Technology</td>\n",
       "      <td>Entry level</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ResultsVersion     ResultsDate             Company  \\\n",
       "0               1  30/07/2021 14h  McKinsey & Company   \n",
       "1               1  30/07/2021 14h             Siemens   \n",
       "2               1  30/07/2021 14h    BOLD by Devoteam   \n",
       "\n",
       "                   Location                Position PostingDate  \\\n",
       "0  Lisbon, Lisbon, Portugal          Data Scientist  2021-07-17   \n",
       "1  Lisbon, Lisbon, Portugal  Data Scientist (m/f/d)  2021-07-03   \n",
       "2          Lisbon, Portugal          Data Scientist  2021-07-30   \n",
       "\n",
       "                                      FullDetailsURL  \\\n",
       "0  https://linkedin.com/jobs/view/data-scientist-...   \n",
       "1  https://linkedin.com/jobs/view/data-scientist-...   \n",
       "2  https://linkedin.com/jobs/view/data-scientist-...   \n",
       "\n",
       "                                   AllQualifications  \\\n",
       "0  ['Master’s degree in a quantitative field like...   \n",
       "1  ['Work with large, complex data sets and apply...   \n",
       "2  ['Bachelor’s degree in the IT area or equivale...   \n",
       "\n",
       "                         Applicants Employment type  \\\n",
       "0                     41 applicants       Full-time   \n",
       "1  Be among the first 25 applicants       Full-time   \n",
       "2                     29 applicants       Full-time   \n",
       "\n",
       "                                          Industries  \\\n",
       "0  Automotive, Aviation & Aerospace, and Manageme...   \n",
       "1                Electrical/Electronic Manufacturing   \n",
       "2                Information Technology and Services   \n",
       "\n",
       "                                        Job function   Seniority level  \n",
       "0  Consulting, Information Technology, and Marketing         Associate  \n",
       "1                             Information Technology  Mid-Senior level  \n",
       "2             Engineering and Information Technology       Entry level  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STAGE 3\n",
    "df_full = gather_full_info(df_extr)\n",
    "df_full.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time: 8.0 min\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Run Time: \" + str('%.1f' % round((end - start) / 60, 1)) + \" min\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Logic to update files:\n",
    "* \"01. DataExtraction\" checks how many temporary files do we have\n",
    "* \"01. DataExtraction\" produces new file with index 0 or max + 1\n",
    "* \"02. DataValidation\" deletes all temporary files from the folder at the end of the day"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
