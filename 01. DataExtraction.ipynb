{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data Wrangling & Other General Use\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# For scrapping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import urllib\n",
    "from urllib import parse\n",
    "\n",
    "\n",
    "# For debugging\n",
    "from icecream import ic\n",
    "ic.configureOutput(prefix = 'Debug | ')\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gathering the page full HTML code (w/ Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_url(keywords_in, location_in):\n",
    "    \"\"\"Pass the parameters to an url parser\"\"\"\n",
    "    querystring = 'search?' + parse.urlencode({'keywords': keywords_in, 'location': location_in, 'position': 1, 'pageNum': 0})\n",
    "    url = 'https://www.linkedin.com/jobs/' + querystring\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_full_html(url):\n",
    "    \"\"\"Gathering the page full HTML code (w/ Selenium)\"\"\"\n",
    "    \n",
    "    print('STAGE 1: Gathering the page full HTML code -----------------------------------------\\n')\n",
    "    \n",
    "    #driver_path = 'C:\\Program Files (x86)\\chromedriver.exe'\n",
    "    driver_path = 'chromedriver.exe'\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "    driver.get(url)\n",
    "\n",
    "    close = 0\n",
    "    while close == 0:\n",
    "        \n",
    "        start_while = time.time() \n",
    "    \n",
    "        # Get the number of jobs the page shows on top of the cards\n",
    "        soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "        try:\n",
    "            # Click the \"Accept Cookies\" button, if it displays\n",
    "            try:\n",
    "                driver.find_element_by_xpath(\"//button[@class='artdeco-global-alert-action artdeco-button artdeco-button--inverse artdeco-button--2 artdeco-button--primary'] \\\n",
    "                                                       and @data-tracking-control-name='ga-cookie.consent.accept.v3'\") \\\n",
    "                      .click()\n",
    "                print('Cookies Accepted.\\n')\n",
    "            except:\n",
    "                pass\n",
    "\n",
    "            nr_jobs = soup.find('span', class_ = 'results-context-header__job-count').text.strip()\n",
    "            print(f'Total Number of Jobs Advertised in the Top: {nr_jobs}\\n')\n",
    "\n",
    "            nr_jobs_initial = get_jobs_loaded(driver)\n",
    "            print('Number of Jobs Loaded in the Browser:')\n",
    "            print(f'  @ Opening Page: {nr_jobs_initial}')\n",
    "\n",
    "            scrolls = 0\n",
    "            buttons = 0\n",
    "\n",
    "            while soup.find('div', class_ = 'inline-notification see-more-jobs__viewed-all') is None:\n",
    "                # Stop when a \"You've viewed all jobs\" card appears\n",
    "\n",
    "                nr_jobs_loaded_init = get_jobs_loaded(driver)\n",
    "\n",
    "                try:\n",
    "                    # Click the \"Show More Jobs\" button\n",
    "                    driver.find_element_by_xpath(\"//button[@class='infinite-scroller__show-more-button infinite-scroller__show-more-button--visible']\").click()\n",
    "                    buttons += 1\n",
    "                    buttons_print = 'Button' if buttons == 1 else 'Buttons'\n",
    "\n",
    "                    # Give the browser some time to fetch the results\n",
    "                    time.sleep(1)\n",
    "\n",
    "                    # Printing the number of jobs already loaded\n",
    "                    nr_jobs_loaded = get_jobs_loaded(driver)\n",
    "                    if nr_jobs_loaded != nr_jobs_loaded_init:\n",
    "                        print(f'  After {buttons} {buttons_print}: {nr_jobs_loaded}')\n",
    "\n",
    "                except:\n",
    "                    \n",
    "                    # Scroll through the infinite scroll until the \"Show More Jobs\" button appears\n",
    "                    driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                    scrolls += 1\n",
    "                    scrolls_print = 'Scroll' if scrolls == 1 else 'Scrolls'\n",
    "\n",
    "                    time.sleep(1.2)\n",
    "                    nr_jobs_loaded = get_jobs_loaded(driver)\n",
    "                    if nr_jobs_loaded != nr_jobs_loaded_init:\n",
    "                        print(f'  After {scrolls} {scrolls_print}: {nr_jobs_loaded}')\n",
    "                \n",
    "                finally:\n",
    "                    # Refreshing the soup for assessment in the while loop condition\n",
    "                    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "                    \n",
    "                    #If for some reason the page is taking too long to load, start again\n",
    "                    if time.time() - start_while > 100:\n",
    "                        print('Taking a while. Maybe it is better to restart.')\n",
    "                        break\n",
    "\n",
    "            # Closing the browser\n",
    "            print(\"\\nBrowser is now closed.\")\n",
    "            driver.close()\n",
    "\n",
    "            print('\\n------------------------------------------------------------------------------------\\n')\n",
    "            \n",
    "            # If we were not blocked, close the loop\n",
    "            close = 1\n",
    "            \n",
    "        except:\n",
    "            driver.close()\n",
    "            sleep_if_blocked = 30\n",
    "            print(f'Linkedin is blocking the crawling. Waiting {sleep_if_blocked} seconds to try again.')\n",
    "            time.sleep(sleep_if_blocked)\n",
    "            \n",
    "    return soup\n",
    "\n",
    "def get_jobs_loaded(driver):\n",
    "    soup_jobs = BeautifulSoup(driver.page_source)\n",
    "    nr_jobs = len(soup_jobs.find('ul', class_ = 'jobs-search__results-list').find_all('li'))\n",
    "    return nr_jobs"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gathering all information from the job cards (w/ BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_job_card_info(soup):\n",
    "    \"\"\"Gathering all information from the job cards (w/ BeautifulSoup)\"\"\"\n",
    "    \n",
    "    print('STAGE 2: Gathering all information from the job cards ------------------------------\\n')\n",
    "    \n",
    "    jobs_card = soup.find('ul', class_ = 'jobs-search__results-list')\n",
    "\n",
    "    jobs = []\n",
    "    repeated_jobs = []\n",
    "    \n",
    "    for li in jobs_card.find_all('li'):\n",
    "        full_details_url = li.find('a').get('href').replace('https://pt.linkedin', 'https://linkedin')\n",
    "        position = li.find('h3', class_ = 'base-search-card__title').text.strip()\n",
    "        company = li.find('h4', class_ = 'base-search-card__subtitle').text.strip()\n",
    "        metadata = li.find('div', class_ = 'base-search-card__metadata')\n",
    "        location = metadata.find('span', class_ = 'job-search-card__location').text.strip()\n",
    "        posting_date = metadata.find('time').get('datetime')\n",
    "\n",
    "        job_info = {'Company': company,\n",
    "                    'Location': location,\n",
    "                    'Position': position,\n",
    "                    'PostingDate': posting_date,\n",
    "                    'FullDetailsURL': full_details_url[:full_details_url.find('?refId=')]}\n",
    "\n",
    "        if job_info not in jobs:\n",
    "            jobs.append(job_info)\n",
    "        else:\n",
    "            if len(repeated_jobs) == 0:\n",
    "                print('Repeated Jobs:')\n",
    "            repeated_jobs.append(job_info)\n",
    "            print(job_info['Company'], '|', job_info['Position'])\n",
    "\n",
    "    df_extr = pd.DataFrame(jobs)\n",
    "\n",
    "    print(f\"\\n{len(jobs)} unique jobs found. Full info now loaded to a dataframe.\")\n",
    "    print('\\n------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    return df_extr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gathering Full Job Info through the URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_full_info(df_extr):\n",
    "    \n",
    "    print(\"STAGE 3: Gathering Full Job Info through the URL's ---------------------------------\\n\")\n",
    "\n",
    "    try:\n",
    "        # Reading previous days info from csv file\n",
    "        df_full = pd.read_csv('FullInfoDataframe.csv') \n",
    "    except:\n",
    "        # First instance of the dataframe \n",
    "        df_full = pd.DataFrame(columns = ['ResultsVersion', 'ResultsDate', 'Company', 'Location',\n",
    "                                          'Position', 'PostingDate', 'FullDetailsURL', 'AllQualifications', 'Applicants'])\n",
    "        df_full.to_csv('FullInfoDataframe.csv',\n",
    "                       index = False,\n",
    "                       encoding = 'utf-8-sig')\n",
    "\n",
    "    print('Fetching results:\\n')\n",
    "    print('JobID | JobTitle | Company')\n",
    "    \n",
    "    last_version = df_full['ResultsVersion'].max() if len(df_full) > 0 else 0\n",
    "\n",
    "    for i in range(len(df_extr)):\n",
    "\n",
    "        job_info = df_extr.iloc[i].to_dict()\n",
    "        # Save the process datetime (day & hour) and a version ID\n",
    "        job_info['ResultsVersion'] = last_version + 1\n",
    "        job_info['ResultsDate'] = datetime.now().strftime(\"%d/%m/%Y %Hh\")\n",
    "\n",
    "        print(i, '|', df_extr['Position'][i], '|', df_extr['Company'][i])\n",
    "\n",
    "        job_url = df_extr['FullDetailsURL'][i]\n",
    "\n",
    "        job_page = requests.get(job_url, headers)\n",
    "        soup = BeautifulSoup(job_page.content, \"lxml\")\n",
    "\n",
    "        try:\n",
    "            # if full_description returns None, we know Linkedin blocked the request\n",
    "            full_description = soup.find('div', class_ = 'show-more-less-html__markup')\n",
    "\n",
    "            try:\n",
    "                # Store required qualifications in a list\n",
    "                qualifications = []\n",
    "                for qualification in full_description.find_all('li'):\n",
    "                    qualification = qualification.text\n",
    "                    qualifications.append(qualification)\n",
    "\n",
    "                job_info['AllQualifications'] = qualifications\n",
    "\n",
    "                try:\n",
    "                    # Job Criteria List (Employment Type, Industries, Job Function, Seniority Level)\n",
    "                    criteria = soup.find('ul', class_ = 'description__job-criteria-list')\n",
    "                    criteria_boxes = criteria.find_all('li', class_ = 'description__job-criteria-item')\n",
    "                    for box in criteria_boxes:\n",
    "                        criteria_header = box.find('h3').text.strip()\n",
    "                        criteria_text = box.find('span').text.strip()\n",
    "\n",
    "                        job_info[criteria_header] = criteria_text\n",
    "\n",
    "                    try:\n",
    "                        # Get the info regarding current applicants\n",
    "                        # If we were logged into Linkedin, we would have the exact number for those jobs under 25 applicants\n",
    "                        try:\n",
    "                            job_info['Applicants'] = soup.find('span', class_ = 'num-applicants__caption topcard__flavor--metadata topcard__flavor--bullet') \\\n",
    "                                                         .text.strip()\n",
    "                        except:\n",
    "                            #job_info['Applicants'] = soup.find('figure', class_ = 'num-applicants__figure topcard__flavor--metadata topcard__flavor--bullet') \\\n",
    "                            #                             .text.strip()\n",
    "                            job_info['Applicants'] = soup.find('figcaption', class_ = 'num-applicants__caption') \\\n",
    "                                                         .text.strip()\n",
    "                            \n",
    "                            \n",
    "                    except:\n",
    "                        print('     Errors occurred when parsing job \"Applicants\"')\n",
    "                except:\n",
    "                    print('     Errors occurred when parsing job \"Criteria\"')\n",
    "            except:\n",
    "                print('     Errors occurred when parsing job \"Qualifications\"')\n",
    "\n",
    "        except:\n",
    "            raise ValueError('LINKEDIN BLOCKED THE REQUEST')\n",
    "\n",
    "        # Add the job dict to the dataframe\n",
    "        df_full = df_full.append(job_info, ignore_index = True)\n",
    "\n",
    "        time.sleep(random.random() * 3 + 1) # Waiting a randomized amount of time (higher than 1 and lower than 4 secs)\n",
    "\n",
    "    df_full.to_csv('FullInfoDataframe.csv',\n",
    "                   index = False,\n",
    "                   encoding = 'utf-8-sig')\n",
    "    \n",
    "    print('\\n------------------------------------------------------------------------------------\\n')\n",
    "    \n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS -------------------------------------------------------\n",
    "# Select the company or the job you want to find results for\n",
    "keywords_in = '\"Data Scientist\"'\n",
    "# Select the location for it\n",
    "location_in = 'Lisbon'\n",
    "# --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 1: Gathering the page full HTML code -----------------------------------------\n",
      "\n",
      "Total Number of Jobs Advertised in the Top: 104\n",
      "\n",
      "Number of Jobs Loaded in the Browser:\n",
      "  @ Opening Page: 25\n",
      "  After 1 Scroll: 49\n",
      "  After 2 Scrolls: 73\n",
      "  After 3 Scrolls: 98\n",
      "  After 4 Scrolls: 102\n",
      "\n",
      "Browser is now closed.\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n"
     ]
    }
   ],
   "source": [
    "# STAGE 1\n",
    "soup = gather_full_html(build_url(keywords_in, location_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 2: Gathering all information from the job cards ------------------------------\n",
      "\n",
      "\n",
      "102 unique jobs found. Full info now loaded to a dataframe.\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Position</th>\n",
       "      <th>PostingDate</th>\n",
       "      <th>FullDetailsURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>McKinsey &amp; Company</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-07-17</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Construo AG</td>\n",
       "      <td>Lisbon, Portugal</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-08-03</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>Siemens</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist (m/f/d)</td>\n",
       "      <td>2021-07-08</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>DataSmart Lda</td>\n",
       "      <td>Lisboa, Lisbon, Portugal</td>\n",
       "      <td>Senior Data Scientist</td>\n",
       "      <td>2021-08-04</td>\n",
       "      <td>https://linkedin.com/jobs/view/senior-data-sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Wyser</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Junior Data Scientist – Scoring Centre (M/F)</td>\n",
       "      <td>2021-08-03</td>\n",
       "      <td>https://linkedin.com/jobs/view/junior-data-sci...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "              Company                  Location  \\\n",
       "0  McKinsey & Company  Lisbon, Lisbon, Portugal   \n",
       "1         Construo AG          Lisbon, Portugal   \n",
       "2             Siemens  Lisbon, Lisbon, Portugal   \n",
       "3       DataSmart Lda  Lisboa, Lisbon, Portugal   \n",
       "4               Wyser  Lisbon, Lisbon, Portugal   \n",
       "\n",
       "                                       Position PostingDate  \\\n",
       "0                                Data Scientist  2021-07-17   \n",
       "1                                Data Scientist  2021-08-03   \n",
       "2                        Data Scientist (m/f/d)  2021-07-08   \n",
       "3                         Senior Data Scientist  2021-08-04   \n",
       "4  Junior Data Scientist – Scoring Centre (M/F)  2021-08-03   \n",
       "\n",
       "                                      FullDetailsURL  \n",
       "0  https://linkedin.com/jobs/view/data-scientist-...  \n",
       "1  https://linkedin.com/jobs/view/data-scientist-...  \n",
       "2  https://linkedin.com/jobs/view/data-scientist-...  \n",
       "3  https://linkedin.com/jobs/view/senior-data-sci...  \n",
       "4  https://linkedin.com/jobs/view/junior-data-sci...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STAGE 2\n",
    "df_extr = gather_job_card_info(soup)\n",
    "df_extr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "STAGE 3: Gathering Full Job Info through the URL's ---------------------------------\n",
      "\n",
      "Fetching results:\n",
      "\n",
      "JobID | JobTitle | Company | Location\n",
      "0 | Data Scientist | McKinsey & Company\n",
      "1 | Data Scientist | Construo AG\n",
      "2 | Data Scientist (m/f/d) | Siemens\n",
      "3 | Senior Data Scientist | DataSmart Lda\n",
      "4 | Junior Data Scientist – Scoring Centre (M/F) | Wyser\n",
      "5 | Product Data Scientist | CASAFARI\n",
      "6 | Data Scientist | SGS\n",
      "7 | Data Scientist – Scoring Centre (M/F) | BNP Paribas Personal Finance\n",
      "8 | Junior Data Scientist | Amgen\n",
      "9 | Senior Data Scientist (Lisbon) | PandaDoc\n",
      "10 | Quantitative Research - Data Scientist | BNP Paribas CIB\n",
      "11 | Data Scientist - Lisboa ou Porto | NOS SGPS\n",
      "12 | Data Scientist Junior (SQL+R) | VASP - Distribuidora de Publicações, S.A.\n",
      "13 | Data Scientist | FPF - Federação Portuguesa de Futebol\n",
      "14 | Data Scientist | everis\n",
      "15 | Senior Data Scientist (all genders) | 4TE\n",
      "16 | Data Scientist – AI & ML (M/F) Lisboa | Adecco Specialized Recruitment\n",
      "17 | Data Scientist (M/F) - Lisbon | Capgemini Engineering\n",
      "18 | Data Manager/Data Scientist (M/F) | JLL\n",
      "19 | Machine Learning Specialist | Experis\n",
      "20 | Senior Data Scientist | KDR Recruitment Ltd\n",
      "21 | Sr. Data Scientist - Bot Management | Cloudflare\n",
      "22 | Data Scientist | Feedzai\n",
      "23 | Data Scientist | Solvay\n",
      "24 | Data Scientist (Nearshore Project in Lisbon) | BOLD by Devoteam\n",
      "25 | Data Scientist | SGS\n",
      "26 | Data Scientist (Traineeship) | Nokia\n",
      "27 | Senior Data Scientist | Faber Talent Pool\n",
      "28 | Data Scientist - m/f | Michael Page\n",
      "29 | Data Scientist [A] (356) | Axians Portugal\n",
      "30 | Senior Data Scientist (Computer Vision/Machine Learning) | Cartrack Portugal\n",
      "31 | Data Scientist - Customer Analytics | Networkers - Technology Recruitment\n",
      "32 | Data Scientist (m/f) | Mind Source\n",
      "33 | Data Scientist (m/f) | Noesis\n",
      "34 | Consultant, Analytics, Data and Services | Mastercard\n",
      "35 | Senior Data Scientist Consultant & Data Lover | Keyrus\n",
      "36 | Data Scientist (m/f) | IT People Innovation\n",
      "37 | SMART DATA SCIENTIST | Lisboa | Smart Consulting\n",
      "38 | Data Scientist (m/f) ? Lisboa | Ankix\n",
      "39 | Anúncio de emprego: Data Scientist (M/F) | askblue\n",
      "40 | Oferta de emprego: Data Scientist | Boost IT\n",
      "41 | Oferta de trabalho Data Scientist (M/F) | Habber Tec\n",
      "42 | Oferta de emprego: Data scientist for International Institution | KCS iT\n",
      "43 | Oferta de emprego: Consultor Data Scientist (m/f) | Match Profiler\n",
      "44 | Data Scientist (French) (m/f) | Integer Consulting\n",
      "45 | Model Risk Manager | Revolut\n",
      "46 | Anúncio de emprego: Data Science- Lisboa | Fyld\n",
      "47 | Epidemiologist | IQVIA\n",
      "48 | Anúncio de emprego: Machine Learning Engineer (m/f) - Nearshore | Passio Consulting\n",
      "49 | Senior Product Designer - AI | Talkdesk\n",
      "50 | Senior Software Developer | Keshtyaar\n",
      "51 | Junior Data Scientist (m/f/d) | Siemens\n",
      "52 | Senior Python Data Engineer (m/f) | Daltix\n",
      "53 | Senior Data Engineer | ConvaTec\n",
      "54 | Senior Data Scientist – Scoring Centre (M/F) | Wyser\n",
      "55 | Junior Data Scientist - Scoring Centre | BNP Paribas Personal Finance\n",
      "56 | Data Scientist | Amgen\n",
      "57 | Risk Data Analyst / Data Scientist | BNP Paribas CIB\n",
      "58 | Data Scientist | BOLD by Devoteam\n",
      "59 | Data Scientist (B) [604] | Axians\n",
      "60 | Data Manager/Data Scientist (M/F) | JLL\n",
      "61 | Data Scientist (A) [621] | Axians Portugal\n",
      "62 | Senior Data Scientist Consultant with Python | Keyrus\n",
      "63 | Anúncio de emprego: Data Scientist | Boost IT\n",
      "64 | Lead Consultant, Analytics, Data and Services | Mastercard\n",
      "65 | Anúncio de emprego: Data Scientist(S) | KCS iT\n",
      "66 | Oferta de trabalho Data Scientist (SAS E-Miner) (F/M) | Habber Tec\n",
      "67 | Capacity Planning Analyst | Cloudflare\n",
      "68 | Oferta de emprego: Data Scientist (m/f) – Lisboa | Ankix\n",
      "69 | Senior Data Engineer | Nokia\n",
      "70 | Real World Data Senior Biostatistician | IQVIA\n",
      "71 | Data Engineer - m/f | Michael Page\n",
      "72 | Data Scientist (A) [621] | Axians\n",
      "73 | Data scientist / Risk Data Analyst Trainee | BNP Paribas CIB\n",
      "74 | Data Scientist | BOLD by Devoteam\n",
      "75 | Data Scientist | Axians Portugal\n",
      "76 | Anúncio de emprego: Data Scientist (SAS E-Miner) (F/M) | Habber Tec\n",
      "77 | Anúncio de emprego: Senior Data Scientist (m/f) – Lisboa | Ankix\n",
      "78 | Capacity Planning Engineer | Cloudflare\n",
      "79 | Data Scientist Intern (m/f/d) | Siemens\n",
      "80 | Procurement Intelligence SAP BW Specialist | Nokia\n",
      "81 | Data Scientist [A] | Axianspt\n",
      "82 | Data Scientist | BOLD by Devoteam\n",
      "83 | Senior Data Scientist / Engineer - Stress Testing and Data Analytics | BNP Paribas CIB\n",
      "84 | Data Scientist (B) [604] | Axians Portugal\n",
      "85 | Capacity Planning Program Manager | Cloudflare\n",
      "86 | Data Scientist | BOLD by Devoteam\n",
      "87 | Data Scientist (m/f) | Findmore\n",
      "88 | Senior Machine Learning | BNP Paribas CIB\n",
      "89 | Data Scientist | BOLD by Devoteam\n",
      "90 | Senior Data Scientist (m/f) | Findmore\n",
      "91 | Data scientist | BOLD by Devoteam\n",
      "92 | Data Scientist (B) [604] | Axianspt\n",
      "93 | Data Scientist (A) [621] | Axianspt\n",
      "94 | Data Scientist | BOLD by Devoteam\n",
      "95 | Data Scientist - Machine Learning & NLP | INOV ? Instituto de Engenharia de Sistemas e Computadores Inovação\n",
      "96 | Data Scientist em NLP | INOV ? Instituto de Engenharia de Sistemas e Computadores Inovação\n",
      "97 | Data Scientist (m/f) | Ankix\n",
      "98 | Data Science & NLP | INOV ? Instituto de Engenharia de Sistemas e Computadores Inovação\n",
      "99 | Anúncio de emprego: Senior Data Scientist | zytics\n",
      "100 | Oferta de emprego: Data Scientist | Vantis - Tecnologias de Informação, Lda\n",
      "101 | BI Consultant (m/f) | Reloading\n",
      "\n",
      "------------------------------------------------------------------------------------\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ResultsVersion</th>\n",
       "      <th>ResultsDate</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Position</th>\n",
       "      <th>PostingDate</th>\n",
       "      <th>FullDetailsURL</th>\n",
       "      <th>AllQualifications</th>\n",
       "      <th>Applicants</th>\n",
       "      <th>Employment type</th>\n",
       "      <th>Industries</th>\n",
       "      <th>Job function</th>\n",
       "      <th>Seniority level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1</td>\n",
       "      <td>30/07/2021 14h</td>\n",
       "      <td>McKinsey &amp; Company</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-07-17</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "      <td>['Master’s degree in a quantitative field like...</td>\n",
       "      <td>41 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Automotive, Aviation &amp; Aerospace, and Manageme...</td>\n",
       "      <td>Consulting, Information Technology, and Marketing</td>\n",
       "      <td>Associate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>30/07/2021 14h</td>\n",
       "      <td>Siemens</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist (m/f/d)</td>\n",
       "      <td>2021-07-03</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "      <td>['Work with large, complex data sets and apply...</td>\n",
       "      <td>Be among the first 25 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Electrical/Electronic Manufacturing</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Mid-Senior level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1</td>\n",
       "      <td>30/07/2021 14h</td>\n",
       "      <td>BOLD by Devoteam</td>\n",
       "      <td>Lisbon, Portugal</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-07-30</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "      <td>['Bachelor’s degree in the IT area or equivale...</td>\n",
       "      <td>29 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Information Technology and Services</td>\n",
       "      <td>Engineering and Information Technology</td>\n",
       "      <td>Entry level</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   ResultsVersion     ResultsDate             Company  \\\n",
       "0               1  30/07/2021 14h  McKinsey & Company   \n",
       "1               1  30/07/2021 14h             Siemens   \n",
       "2               1  30/07/2021 14h    BOLD by Devoteam   \n",
       "\n",
       "                   Location                Position PostingDate  \\\n",
       "0  Lisbon, Lisbon, Portugal          Data Scientist  2021-07-17   \n",
       "1  Lisbon, Lisbon, Portugal  Data Scientist (m/f/d)  2021-07-03   \n",
       "2          Lisbon, Portugal          Data Scientist  2021-07-30   \n",
       "\n",
       "                                      FullDetailsURL  \\\n",
       "0  https://linkedin.com/jobs/view/data-scientist-...   \n",
       "1  https://linkedin.com/jobs/view/data-scientist-...   \n",
       "2  https://linkedin.com/jobs/view/data-scientist-...   \n",
       "\n",
       "                                   AllQualifications  \\\n",
       "0  ['Master’s degree in a quantitative field like...   \n",
       "1  ['Work with large, complex data sets and apply...   \n",
       "2  ['Bachelor’s degree in the IT area or equivale...   \n",
       "\n",
       "                         Applicants Employment type  \\\n",
       "0                     41 applicants       Full-time   \n",
       "1  Be among the first 25 applicants       Full-time   \n",
       "2                     29 applicants       Full-time   \n",
       "\n",
       "                                          Industries  \\\n",
       "0  Automotive, Aviation & Aerospace, and Manageme...   \n",
       "1                Electrical/Electronic Manufacturing   \n",
       "2                Information Technology and Services   \n",
       "\n",
       "                                        Job function   Seniority level  \n",
       "0  Consulting, Information Technology, and Marketing         Associate  \n",
       "1                             Information Technology  Mid-Senior level  \n",
       "2             Engineering and Information Technology       Entry level  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STAGE 3\n",
    "df_full = gather_full_info(df_extr)\n",
    "df_full.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Run Time: 7.9 min\n"
     ]
    }
   ],
   "source": [
    "end = time.time()\n",
    "print(\"Run Time: \" + str('%.1f' % round((end - start) / 60, 1)) + \" min\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['01. DataExtraction.ipynb',\n",
       " '02. Data Analysis.ipynb',\n",
       " '02. DataAnalysis.ipynb',\n",
       " '02. DataValidation.ipynb',\n",
       " 'chromedriver.exe',\n",
       " 'FullInfoDataframe.csv',\n",
       " 'README.md']"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "from os.path import isfile, join\n",
    "\n",
    "working_path = os.getcwd()\n",
    "\n",
    "onlyfiles = [f for f in os.listdir(working_path) if isfile(join(working_path, f))]\n",
    "onlyfiles"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
