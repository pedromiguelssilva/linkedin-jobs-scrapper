{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: selenium in c:\\users\\897760\\anaconda3\\lib\\site-packages (3.141.0)\n",
      "Requirement already satisfied: urllib3 in c:\\users\\897760\\anaconda3\\lib\\site-packages (from selenium) (1.25.8)\n",
      "Requirement already satisfied: icecream in c:\\users\\897760\\anaconda3\\lib\\site-packages (2.1.1)\n",
      "Requirement already satisfied: colorama>=0.3.9 in c:\\users\\897760\\anaconda3\\lib\\site-packages (from icecream) (0.4.3)\n",
      "Requirement already satisfied: executing>=0.3.1 in c:\\users\\897760\\anaconda3\\lib\\site-packages (from icecream) (0.7.0)\n",
      "Requirement already satisfied: pygments>=2.2.0 in c:\\users\\897760\\anaconda3\\lib\\site-packages (from icecream) (2.5.2)\n",
      "Requirement already satisfied: asttokens>=2.0.1 in c:\\users\\897760\\anaconda3\\lib\\site-packages (from icecream) (2.0.5)\n",
      "Requirement already satisfied: six in c:\\users\\897760\\anaconda3\\lib\\site-packages (from asttokens>=2.0.1->icecream) (1.14.0)\n"
     ]
    }
   ],
   "source": [
    "# Data Wrangling & Other General Use\n",
    "import pandas as pd\n",
    "import time\n",
    "import random\n",
    "from datetime import datetime\n",
    "\n",
    "# For scrapping\n",
    "import requests\n",
    "from bs4 import BeautifulSoup\n",
    "from selenium import webdriver\n",
    "import urllib\n",
    "from urllib import parse\n",
    "\n",
    "\n",
    "# For debugging\n",
    "from icecream import ic\n",
    "ic.configureOutput(prefix = 'Debug | ')\n",
    "\n",
    "headers = {'User-Agent': 'Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/91.0.4472.114 Safari/537.36'}\n",
    "start = time.time()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 1. Gathering the page full HTML code (w/ Selenium)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def build_url(keywords_in, location_in):\n",
    "    \"\"\"Pass the parameters to an url parser\"\"\"\n",
    "    querystring = 'search?' + parse.urlencode({'keywords': keywords_in, 'location': location_in, 'position': 1, 'pageNum': 0})\n",
    "    url = 'https://www.linkedin.com/jobs/' + querystring\n",
    "    return url"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_full_html(url):\n",
    "    \"\"\"Gathering the page full HTML code (w/ Selenium)\"\"\"\n",
    "    \n",
    "    #driver_path = 'C:\\Program Files (x86)\\chromedriver.exe'\n",
    "    driver_path = 'chromedriver.exe'\n",
    "    driver = webdriver.Chrome(driver_path)\n",
    "    driver.get(url)\n",
    "\n",
    "    # Get the number of jobs the page shows on top of the cards\n",
    "    soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "    \n",
    "    try:\n",
    "        # Click the \"Accept Cookies\" button, if it displays\n",
    "        try:\n",
    "            driver.find_element_by_xpath(\"//button[@class='artdeco-global-alert-action artdeco-button artdeco-button--inverse artdeco-button--2 artdeco-button--primary'] \\\n",
    "                                                   and @data-tracking-control-name='ga-cookie.consent.accept.v3'\") \\\n",
    "                  .click()\n",
    "            print('Cookies Accepted.\\n')\n",
    "        except:\n",
    "            pass\n",
    "        \n",
    "        nr_jobs = soup.find('span', class_ = 'results-context-header__job-count').text.strip()\n",
    "        print(f'\\nTotal Number of Jobs Advertised in the Top: {nr_jobs}\\n')\n",
    "\n",
    "        nr_jobs_initial = get_jobs_loaded(driver)\n",
    "        print('Number of Jobs Loaded in the Browser:')\n",
    "        print(f'  @ Opening Page: {nr_jobs_initial}')\n",
    "\n",
    "        scrolls = 0\n",
    "        buttons = 0\n",
    "\n",
    "        while soup.find('div', class_ = 'inline-notification see-more-jobs__viewed-all') is None:\n",
    "            # Stop when a \"You've viewed all jobs\" card appears\n",
    "\n",
    "            nr_jobs_loaded_init = get_jobs_loaded(driver)\n",
    "\n",
    "            try:\n",
    "                # Click the \"Show More Jobs\" button\n",
    "                driver.find_element_by_xpath(\"//button[@class='infinite-scroller__show-more-button infinite-scroller__show-more-button--visible']\").click()\n",
    "                buttons += 1\n",
    "                buttons_print = 'Button' if buttons == 1 else 'Buttons'\n",
    "\n",
    "                # Give the browser some time to fetch the results\n",
    "                time.sleep(1)\n",
    "\n",
    "                # Printing the number of jobs already loaded\n",
    "                nr_jobs_loaded = get_jobs_loaded(driver)\n",
    "                if nr_jobs_loaded != nr_jobs_loaded_init:\n",
    "                    print(f'  After {buttons} {buttons_print}: {nr_jobs_loaded}')\n",
    "\n",
    "            except:\n",
    "                # Scroll through the infinite scroll until the \"Show More Jobs\" button appears\n",
    "                driver.execute_script(\"window.scrollTo(0, document.body.scrollHeight);\")\n",
    "                scrolls += 1\n",
    "                scrolls_print = 'Scroll' if scrolls == 1 else 'Scrolls'\n",
    "\n",
    "                time.sleep(1.2)\n",
    "                nr_jobs_loaded = get_jobs_loaded(driver)\n",
    "                if nr_jobs_loaded != nr_jobs_loaded_init:\n",
    "                    print(f'  After {scrolls} {scrolls_print}: {nr_jobs_loaded}')\n",
    "\n",
    "\n",
    "            # Refreshing the soup for assessment in the while loop condition\n",
    "            soup = BeautifulSoup(driver.page_source, \"lxml\")\n",
    "\n",
    "        # Closing the browser\n",
    "        print(\"\\nBrowser is now closed.\")\n",
    "        driver.close()\n",
    "    \n",
    "    except:\n",
    "        raise ValueError('Linkedin is blocking the crawling. Wait some more and try again.')\n",
    "        driver.close()\n",
    "    \n",
    "    return soup"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 2. Gathering all information from the job cards (w/ BeautifulSoup)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_job_card_info(soup):\n",
    "    \"\"\"Gathering all information from the job cards (w/ BeautifulSoup)\"\"\"\n",
    "    \n",
    "    jobs_card = soup.find('ul', class_ = 'jobs-search__results-list')\n",
    "\n",
    "    jobs = []\n",
    "\n",
    "    for li in jobs_card.find_all('li'):\n",
    "        full_details_url = li.find('a').get('href').replace('https://pt.linkedin', 'https://linkedin')\n",
    "        position = li.find('h3', class_ = 'base-search-card__title').text.strip()\n",
    "        company = li.find('h4', class_ = 'base-search-card__subtitle').text.strip()\n",
    "        metadata = li.find('div', class_ = 'base-search-card__metadata')\n",
    "        location = metadata.find('span', class_ = 'job-search-card__location').text.strip()\n",
    "        posting_date = metadata.find('time').get('datetime')\n",
    "\n",
    "        job_info = {'Company': company,\n",
    "                    'Location': location,\n",
    "                    'Position': position,\n",
    "                    'PostingDate': posting_date,\n",
    "                    'FullDetailsURL': full_details_url[:full_details_url.find('?refId=')]}\n",
    "\n",
    "        if job_info not in jobs:\n",
    "            jobs.append(job_info)\n",
    "        else:\n",
    "            print(job_info['Company'], '|', job_info['Position'])\n",
    "\n",
    "    df_extr = pd.DataFrame(jobs)\n",
    "\n",
    "    print(f\"\\nAll {len(jobs)} jobs' information is now loaded to a dataframe.\\n\")\n",
    "    \n",
    "    return df_extr"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## 3. Gathering Full Job Info through the URL's"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gather_full_info(df_extr):\n",
    "\n",
    "    try:\n",
    "        # Reading previous days info from csv file\n",
    "        df_full = pd.read_csv('FullInfoDataframe.csv') \n",
    "    except:\n",
    "        # First instance of the dataframe \n",
    "        df_full = pd.DataFrame(columns = ['ResultsDate', 'Company', 'Location', 'Position', 'PostingDate', 'FullDetailsURL', 'AllQualifications', 'Applicants'])\n",
    "        df_full.to_csv('FullInfoDataframe.csv',\n",
    "                       index = False)\n",
    "\n",
    "    print('Fetching results:\\n')\n",
    "    print('JobID | JobTitle | Company | Location')\n",
    "\n",
    "    for i in range(len(df_extr)):\n",
    "\n",
    "        if df_extr['FullDetailsURL'][i] not in df_full['FullDetailsURL'].unique():\n",
    "\n",
    "            job_info = df_extr.iloc[i].to_dict()\n",
    "            # Save the process datetime (day & hour)\n",
    "            job_info['ResultsDate'] = datetime.now().strftime(\"%d/%m/%Y %Hh\")\n",
    "\n",
    "            print(i, '|', df_extr['Position'][i], '|', df_extr['Company'][i])\n",
    "\n",
    "            job_url = df_extr['FullDetailsURL'][i]\n",
    "\n",
    "            job_page = requests.get(job_url, headers)\n",
    "            soup = BeautifulSoup(job_page.content, \"lxml\")\n",
    "\n",
    "            try:\n",
    "                # if full_description returns None, we know Linkedin blocked the request\n",
    "                full_description = soup.find('div', class_ = 'show-more-less-html__markup')\n",
    "\n",
    "                try:\n",
    "                    # Store required qualifications in a list\n",
    "                    qualifications = []\n",
    "                    for qualification in full_description.find_all('li'):\n",
    "                        qualification = qualification.text\n",
    "                        qualifications.append(qualification)\n",
    "\n",
    "                    job_info['AllQualifications'] = qualifications\n",
    "\n",
    "                    try:\n",
    "                        # Job Criteria List (Employment Type, Industries, Job Function, Seniority Level)\n",
    "                        criteria = soup.find('ul', class_ = 'description__job-criteria-list')\n",
    "                        criteria_boxes = criteria.find_all('li', class_ = 'description__job-criteria-item')\n",
    "                        for box in criteria_boxes:\n",
    "                            criteria_header = box.find('h3').text.strip()\n",
    "                            criteria_text = box.find('span').text.strip()\n",
    "\n",
    "                            job_info[criteria_header] = criteria_text\n",
    "\n",
    "                        try:\n",
    "                            # Get the info regarding current applicants\n",
    "                            # If we were logged into Linkedin, we would have the exact number for those jobs under 25 applicants\n",
    "                            try:\n",
    "                                job_info['Applicants'] = soup.find('span', class_ = 'num-applicants__caption topcard__flavor--metadata topcard__flavor--bullet') \\\n",
    "                                                             .text.strip()\n",
    "                            except:\n",
    "                                job_info['Applicants'] = soup.find('figure', class_ = 'num-applicants__figure topcard__flavor--metadata topcard__flavor--bullet') \\\n",
    "                                                             .text.strip()\n",
    "\n",
    "                        except:\n",
    "                            print('     Errors occurred when parsing job \"Applicants\"')\n",
    "                    except:\n",
    "                        print('     Errors occurred when parsing job \"Criteria\"')\n",
    "                except:\n",
    "                    print('     Errors occurred when parsing job \"Qualifications\"')\n",
    "\n",
    "            except:\n",
    "                raise ValueError('LINKEDIN BLOCKED THE REQUEST')\n",
    "\n",
    "            # Add the job dict to the dataframe\n",
    "            df_full = df_full.append(job_info, ignore_index = True)\n",
    "\n",
    "        time.sleep(random.random() * 3 + 1) # Waiting a randomized amount of time (higher than 1 and lower than 4 secs)\n",
    "\n",
    "    df_full.to_csv('FullInfoDataframe.csv',\n",
    "                   index = False)\n",
    "    \n",
    "    return df_full"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Running the whole process"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# INPUTS -------------------------------------------------------\n",
    "# Select the company or the job you want to find results for\n",
    "keywords_in = '\"Data Scientist\"'\n",
    "# Select the location for it\n",
    "location_in = 'Lisbon'\n",
    "# --------------------------------------------------------------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "Total Number of Jobs Advertised in the Top: 111\n",
      "\n",
      "Number of Jobs Loaded in the Browser:\n",
      "  @ Opening Page: 24\n",
      "  After 1 Scroll: 48\n",
      "  After 2 Scrolls: 72\n",
      "  After 3 Scrolls: 97\n",
      "  After 4 Scrolls: 108\n",
      "\n",
      "Browser is now closed.\n"
     ]
    }
   ],
   "source": [
    "# STAGE 1\n",
    "soup = gather_full_html(build_url(keywords_in, location_in))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "All 108 jobs' information is now loaded to a dataframe.\n",
      "\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Position</th>\n",
       "      <th>PostingDate</th>\n",
       "      <th>FullDetailsURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>BNP Paribas CIB</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Senior Data Scientist / Engineer - Stress Test...</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>https://linkedin.com/jobs/view/senior-data-sci...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>Axianspt</td>\n",
       "      <td>Lisboa, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-07-13</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>JLL</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Manager/Data Scientist (M/F)</td>\n",
       "      <td>2021-05-23</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-manager-da...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Smart Consulting</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist (m/f)</td>\n",
       "      <td>2021-06-25</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>Noesis</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist (m/f)</td>\n",
       "      <td>2021-07-24</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            Company                  Location  \\\n",
       "0   BNP Paribas CIB  Lisbon, Lisbon, Portugal   \n",
       "1          Axianspt  Lisboa, Lisbon, Portugal   \n",
       "2               JLL  Lisbon, Lisbon, Portugal   \n",
       "3  Smart Consulting  Lisbon, Lisbon, Portugal   \n",
       "4            Noesis  Lisbon, Lisbon, Portugal   \n",
       "\n",
       "                                            Position PostingDate  \\\n",
       "0  Senior Data Scientist / Engineer - Stress Test...  2021-07-01   \n",
       "1                                     Data Scientist  2021-07-13   \n",
       "2                  Data Manager/Data Scientist (M/F)  2021-05-23   \n",
       "3                               Data Scientist (m/f)  2021-06-25   \n",
       "4                               Data Scientist (m/f)  2021-07-24   \n",
       "\n",
       "                                      FullDetailsURL  \n",
       "0  https://linkedin.com/jobs/view/senior-data-sci...  \n",
       "1  https://linkedin.com/jobs/view/data-scientist-...  \n",
       "2  https://linkedin.com/jobs/view/data-manager-da...  \n",
       "3  https://linkedin.com/jobs/view/data-scientist-...  \n",
       "4  https://linkedin.com/jobs/view/data-scientist-...  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STAGE 2\n",
    "df_extr = gather_job_card_info(soup)\n",
    "df_extr.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fetching results:\n",
      "\n",
      "JobID | JobTitle | Company | Location\n",
      "0 | Senior Data Scientist / Engineer - Stress Testing and Data Analytics | BNP Paribas CIB\n",
      "1 | Data Scientist | Axianspt\n",
      "2 | Data Manager/Data Scientist (M/F) | JLL\n",
      "3 | Data Scientist (m/f) | Smart Consulting\n",
      "4 | Data Scientist (m/f) | Noesis\n",
      "5 | Senior Data Scientist | Feedzai\n",
      "6 | Data Scientist (M/F) - Lisbon | Capgemini Engineering\n",
      "7 | Data Scientist (Traineeship) | Nokia\n",
      "8 | Senior Data Scientist Consultant & Data Lover | Keyrus\n",
      "9 | Data Scientist | SGS\n",
      "10 | Sr. Data Scientist - Bot Management | Cloudflare\n",
      "11 | Data Scientist (Nearshore Project in Lisbon) | BOLD by Devoteam\n",
      "12 | Data Scientist | Axians Portugal\n",
      "13 | Anúncio de emprego: Data Scientist | Boost IT\n",
      "14 | Epidemiologist | IQVIA\n",
      "15 | Senior Data Scientist (m/f) | Findmore\n",
      "16 | Data Scientist (A) [621] | Axians Portugal\n",
      "17 | Real World Data Senior Biostatistician | IQVIA\n",
      "18 | Oferta de emprego: Data Scientist | Vantis - Tecnologias de Informação, Lda\n",
      "19 | Data Scientist (B) [604] | Axianspt\n",
      "20 | Senior Data Scientist Consultant with Python | Keyrus\n",
      "21 | Senior Data Scientist | KDR Recruitment Ltd\n",
      "22 | Data Science & NLP | INOV ? Instituto de Engenharia de Sistemas e Computadores Inovação\n",
      "23 | New Grads - PT Remote | OutSystems\n",
      "24 | Consultant, Analytics, Data and Services | Mastercard\n",
      "25 | Data Scientist Intern (m/f/d) | Siemens\n",
      "26 | Data Scientist (A) [621] | Axianspt\n",
      "27 | Data Scientist | Amgen\n",
      "28 | Risk Data Analyst / Data Scientist | BNP Paribas CIB\n",
      "29 | Data Scientist – Remote, Full-time | Toptal\n",
      "30 | SMART DATA SCIENTIST | Lisboa | Smart Consulting\n",
      "31 | Data Scientist | everis\n",
      "32 | Junior Data Scientist | Amgen\n",
      "33 | Data Manager/Data Scientist (M/F) | JLL\n",
      "34 | Data Scientist | BOLD by Devoteam\n",
      "35 | Anúncio de emprego: Senior Data Scientist | zytics\n",
      "36 | Data Scientist – Scoring Centre (M/F) | BNP Paribas Personal Finance\n",
      "     Errors occurred when parsing job \"Qualifications\"\n",
      "37 | Senior Data Scientist | Findmore\n",
      "38 | Anúncio de emprego: Data Science- Lisboa | Fyld\n",
      "39 | Data Scientist [A] | Axianspt\n",
      "40 | Data Scientist (m/f) | Ankix\n",
      "41 | Senior Data Scientist (Computer Vision/Machine Learning) | Cartrack Portugal\n",
      "42 | Real World Data Senior Biostatistician | IQVIA\n",
      "43 | Oferta de trabalho Data Scientist (M/F) | Habber Tec\n",
      "44 | Oferta de emprego: Data Scientist (m/f) – Lisboa | Ankix\n",
      "45 | Data Scientist (m/f) ? Lisboa | Ankix\n",
      "46 | Data Scientist | Feedzai\n",
      "47 | Data Scientist | BOLD by Devoteam\n",
      "     Errors occurred when parsing job \"Applicants\"\n",
      "48 | Senior Data Engineer | Nokia\n",
      "49 | Anúncio de emprego: Data Scientist (SAS E-Miner) (F/M) | Habber Tec\n",
      "50 | Oferta de emprego: Data scientist for International Institution | KCS iT\n",
      "51 | Quantitative Research - Data Scientist | BNP Paribas CIB\n",
      "52 | Lead Consultant, Analytics, Data and Services | Mastercard\n",
      "53 | Senior Data Scientist | Faber Talent Pool\n",
      "54 | Data Scientist | McKinsey & Company\n",
      "55 | Anúncio de emprego: Senior Data Scientist (m/f) – Lisboa | Ankix\n",
      "56 | Data Scientist | BOLD by Devoteam\n",
      "     Errors occurred when parsing job \"Qualifications\"\n",
      "57 | Data Scientist (B) [604] | Axians Portugal\n",
      "58 | Data Scientist – Remote, Full-time | Toptal\n",
      "59 | Anúncio de emprego: Data Scientist (M/F) | askblue\n",
      "60 | Data Scientist | BOLD by Devoteam\n",
      "     Errors occurred when parsing job \"Applicants\"\n",
      "61 | Data Scientist - Lisboa ou Porto | NOS SGPS\n",
      "62 | Data Scientist (m/f) | IT People Innovation\n",
      "63 | Oferta de emprego: Data Scientist | Boost IT\n",
      "64 | Sr Data Scientist | Nokia\n",
      "65 | Capacity Planning Analyst | Cloudflare\n",
      "66 | Data Scientist (m/f) | Mind Source\n",
      "67 | Product Data Scientist | CASAFARI\n",
      "68 | Junior Data Scientist – Scoring Centre (M/F) | Wyser\n",
      "69 | Data Scientist - Machine Learning & NLP | INOV ? Instituto de Engenharia de Sistemas e Computadores Inovação\n",
      "70 | Oferta de emprego: Consultor Data Scientist (m/f) | Match Profiler\n",
      "71 | Data Scientist em NLP | INOV ? Instituto de Engenharia de Sistemas e Computadores Inovação\n",
      "72 | Procurement Intelligence SAP BW Specialist | Nokia\n",
      "73 | Data Scientist | BOLD by Devoteam\n",
      "     Errors occurred when parsing job \"Applicants\"\n",
      "74 | Junior Data Scientist (m/f/d) | Siemens\n",
      "75 | Data Scientist (m/f) | Findmore\n",
      "76 | BI Consultant (m/f) | Reloading\n",
      "77 | Senior Data Scientist (Lisbon) | PandaDoc\n",
      "78 | Data Scientist (French) (m/f) | Integer Consulting\n",
      "79 | Data Scientist - m/f | Michael Page\n",
      "80 | Data Engineer - m/f | Michael Page\n",
      "81 | Data Scientist [A] (356) | Axians Portugal\n",
      "82 | IBM Associates Business Transformation Consultant | IBM\n",
      "83 | Data Scientist | Winning\n",
      "84 | CUSTOMER INTELLIGENCE ANALYST – Data Scientist – Lisbon | BNP Paribas Personal Finance\n",
      "85 | Data Scientist | Solvay\n",
      "86 | Junior Data Scientist - Scoring Centre | BNP Paribas Personal Finance\n",
      "87 | Senior Data Engineer | ConvaTec\n",
      "88 | Business Intelligence Developer | SQS Portugal\n",
      "89 | Data Scientist (m/f/d) | Siemens\n",
      "90 | Capacity Planning Program Manager | Cloudflare\n",
      "91 | Senior Product Designer - AI | Talkdesk\n",
      "92 | Senior Python Data Engineer (m/f) | Daltix\n",
      "93 | Oferta de trabalho Data Scientist (SAS E-Miner) (F/M) | Habber Tec\n",
      "94 | Anúncio de emprego: Machine Learning Engineer (m/f) - Nearshore | Passio Consulting\n",
      "95 | Data Scientist – AI & ML (M/F) Lisboa | Adecco Specialized Recruitment\n",
      "96 | Consultor Data Scientist Machine Learning (m/f) | Bee Engineering ICT\n",
      "97 | Data Scientist | PrimeIT\n",
      "98 | Data Scientist (m/f) | QiBit Portugal\n",
      "99 | Senior Data Scientist – Scoring Centre (M/F) | Wyser\n",
      "100 | Data scientist / Risk Data Analyst Trainee | BNP Paribas CIB\n",
      "101 | Model Risk Manager | Revolut\n",
      "102 | Data Scientist - Customer Analytics | Networkers - Technology Recruitment\n",
      "103 | Data scientist | BOLD by Devoteam\n",
      "104 | Data Scientist | BOLD by Devoteam\n",
      "105 | Capacity Planning Engineer | Cloudflare\n",
      "106 | Data Scientist | Findmore\n",
      "107 | Anúncio de emprego: Data Scientist(S) | KCS iT\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>ResultsDate</th>\n",
       "      <th>Company</th>\n",
       "      <th>Location</th>\n",
       "      <th>Position</th>\n",
       "      <th>PostingDate</th>\n",
       "      <th>FullDetailsURL</th>\n",
       "      <th>AllQualifications</th>\n",
       "      <th>Applicants</th>\n",
       "      <th>Employment type</th>\n",
       "      <th>Industries</th>\n",
       "      <th>Job function</th>\n",
       "      <th>Seniority level</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>27/07/2021 17h</td>\n",
       "      <td>BNP Paribas CIB</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Senior Data Scientist / Engineer - Stress Test...</td>\n",
       "      <td>2021-07-01</td>\n",
       "      <td>https://linkedin.com/jobs/view/senior-data-sci...</td>\n",
       "      <td>[BNP Paribas is a leader in the Eurozone, and ...</td>\n",
       "      <td>Be among the first 25 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Information Technology and Services, Banking, ...</td>\n",
       "      <td>Engineering and Information Technology</td>\n",
       "      <td>Associate</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>27/07/2021 17h</td>\n",
       "      <td>Axianspt</td>\n",
       "      <td>Lisboa, Lisbon, Portugal</td>\n",
       "      <td>Data Scientist</td>\n",
       "      <td>2021-07-13</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-scientist-...</td>\n",
       "      <td>[]</td>\n",
       "      <td>Be among the first 25 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Information Technology and Services, Computer ...</td>\n",
       "      <td>Engineering and Information Technology</td>\n",
       "      <td>Entry level</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>27/07/2021 17h</td>\n",
       "      <td>JLL</td>\n",
       "      <td>Lisbon, Lisbon, Portugal</td>\n",
       "      <td>Data Manager/Data Scientist (M/F)</td>\n",
       "      <td>2021-05-23</td>\n",
       "      <td>https://linkedin.com/jobs/view/data-manager-da...</td>\n",
       "      <td>[ Execute tasks within the full lifecycle on m...</td>\n",
       "      <td>Be among the first 25 applicants</td>\n",
       "      <td>Full-time</td>\n",
       "      <td>Commercial Real Estate, Management Consulting,...</td>\n",
       "      <td>Information Technology</td>\n",
       "      <td>Associate</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "      ResultsDate          Company                  Location  \\\n",
       "0  27/07/2021 17h  BNP Paribas CIB  Lisbon, Lisbon, Portugal   \n",
       "1  27/07/2021 17h         Axianspt  Lisboa, Lisbon, Portugal   \n",
       "2  27/07/2021 17h              JLL  Lisbon, Lisbon, Portugal   \n",
       "\n",
       "                                            Position PostingDate  \\\n",
       "0  Senior Data Scientist / Engineer - Stress Test...  2021-07-01   \n",
       "1                                     Data Scientist  2021-07-13   \n",
       "2                  Data Manager/Data Scientist (M/F)  2021-05-23   \n",
       "\n",
       "                                      FullDetailsURL  \\\n",
       "0  https://linkedin.com/jobs/view/senior-data-sci...   \n",
       "1  https://linkedin.com/jobs/view/data-scientist-...   \n",
       "2  https://linkedin.com/jobs/view/data-manager-da...   \n",
       "\n",
       "                                   AllQualifications  \\\n",
       "0  [BNP Paribas is a leader in the Eurozone, and ...   \n",
       "1                                                 []   \n",
       "2  [ Execute tasks within the full lifecycle on m...   \n",
       "\n",
       "                         Applicants Employment type  \\\n",
       "0  Be among the first 25 applicants       Full-time   \n",
       "1  Be among the first 25 applicants       Full-time   \n",
       "2  Be among the first 25 applicants       Full-time   \n",
       "\n",
       "                                          Industries  \\\n",
       "0  Information Technology and Services, Banking, ...   \n",
       "1  Information Technology and Services, Computer ...   \n",
       "2  Commercial Real Estate, Management Consulting,...   \n",
       "\n",
       "                             Job function Seniority level  \n",
       "0  Engineering and Information Technology       Associate  \n",
       "1  Engineering and Information Technology     Entry level  \n",
       "2                  Information Technology       Associate  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# STAGE 3\n",
    "df_full = gather_full_info(df_extr)\n",
    "df_full.head(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "end = time.time()\n",
    "print(\"Run Time: \" + str('%.0f' % round(end - start,0)) + \"s\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "TO DO:\n",
    "1. Schedule job to run the script at least daily\n",
    "2. Write script 2 - Data Analysis"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
